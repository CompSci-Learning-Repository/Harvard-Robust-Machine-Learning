{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "We introduce a 3-layer perceptron, i.e. a 3 layer fully connected neural network. We do this using the MNIST data once again. We first write the model as we were doing before. Later, we will show how we can cleanly organize the functions to a Python `Class`. It's time to get object oriented :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = mnist.train.images.shape[1]\n",
    "num_classes  = mnist.train.labels.shape[1]\n",
    "num_hidden_1 = 256\n",
    "num_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, num_features], name='Input-Images')\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, num_classes], name='Output-Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('fc1'):\n",
    "    W1 = tf.Variable(tf.random_normal([num_features, num_hidden_1]),name='weights')\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden_1]),name='bias')\n",
    "\n",
    "with tf.name_scope('fc2'):\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2]),name='weights')\n",
    "    b2 = tf.Variable(tf.random_normal([num_hidden_2]),name='bias')\n",
    "\n",
    "with tf.name_scope('out'):\n",
    "    Wout = tf.Variable(tf.random_normal([num_hidden_2, num_classes]),name='weights')\n",
    "    bout = tf.Variable(tf.random_normal([num_classes]),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('multilayer_perceptron'):\n",
    "    H1 = tf.nn.relu(X @ W1 + b1, name='H1')\n",
    "    H2 = tf.nn.relu(H1 @ W2 + b2, name='H2')\n",
    "    logits = tf.add(H2 @ Wout, bout, name='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                             logits=logits, labels=Y),name='loss')\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 61.8643349510972\n",
      "Epoch: 1 Cost: 12.187720913453521\n",
      "Epoch: 2 Cost: 7.424538441219112\n",
      "Epoch: 3 Cost: 5.4271530052884\n",
      "Epoch: 4 Cost: 3.982053814406861\n",
      "Epoch: 5 Cost: 3.151202641244398\n",
      "Epoch: 6 Cost: 2.6753018941547957\n",
      "Epoch: 7 Cost: 2.1888300399692033\n",
      "Epoch: 8 Cost: 1.813328645066876\n",
      "Epoch: 9 Cost: 1.4697528642732964\n",
      "Epoch: 10 Cost: 1.3372768463187745\n",
      "Epoch: 11 Cost: 1.2398637867008184\n",
      "Epoch: 12 Cost: 0.9818570881490868\n",
      "Epoch: 13 Cost: 0.8850345878592402\n",
      "Epoch: 14 Cost: 0.7661460467218224\n",
      "Epoch: 15 Cost: 0.7395797746256713\n",
      "Epoch: 16 Cost: 0.5951993956270213\n",
      "Epoch: 17 Cost: 0.5810083896863325\n",
      "Epoch: 18 Cost: 0.4626998142226008\n",
      "Epoch: 19 Cost: 0.44251203135753675\n",
      "Epoch: 20 Cost: 0.4166008296509719\n",
      "Epoch: 21 Cost: 0.35637803889022446\n",
      "Epoch: 22 Cost: 0.30885852929234\n",
      "Epoch: 23 Cost: 0.29982214903542986\n",
      "Epoch: 24 Cost: 0.2562230518264089\n",
      "Test Accuracy: 0.9276\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('log/multilayer_perceptron1', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0\n",
    "        for batch in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([update, loss], feed_dict={X: batch_X,\n",
    "                                                       Y: batch_Y})\n",
    "            average_cost += c / total_batch\n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                      Y: batch_Y})\n",
    "            global_step = epoch*total_batch + batch\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "    \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, num_features], name='Input-Images')\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, num_classes], name='Output-Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('multilayer_perceptron'):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(X, num_hidden_1, tf.nn.relu, name='fc1')\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, num_hidden_2, tf.nn.relu, name='fc2')\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    logits = tf.layers.dense(layer_2, num_classes, name='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                             logits=logits, labels=Y),name='loss')\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 1.1429620449651363\n",
      "Epoch: 1 Cost: 0.44573915083299925\n",
      "Epoch: 2 Cost: 0.35664164621721617\n",
      "Epoch: 3 Cost: 0.31739128164269714\n",
      "Epoch: 4 Cost: 0.2910888133265754\n",
      "Epoch: 5 Cost: 0.2713912793858482\n",
      "Epoch: 6 Cost: 0.25498610958456974\n",
      "Epoch: 7 Cost: 0.2407954240658066\n",
      "Epoch: 8 Cost: 0.22825071882117884\n",
      "Epoch: 9 Cost: 0.21688424812121837\n",
      "Epoch: 10 Cost: 0.20677164849909882\n",
      "Epoch: 11 Cost: 0.19733745005320408\n",
      "Epoch: 12 Cost: 0.1889122671769422\n",
      "Epoch: 13 Cost: 0.18079517665234468\n",
      "Epoch: 14 Cost: 0.17358795196495266\n",
      "Epoch: 15 Cost: 0.16686928619037983\n",
      "Epoch: 16 Cost: 0.1604892960935832\n",
      "Epoch: 17 Cost: 0.15470593801953564\n",
      "Epoch: 18 Cost: 0.14925623104653593\n",
      "Epoch: 19 Cost: 0.14406697281382297\n",
      "Epoch: 20 Cost: 0.1391177260740237\n",
      "Epoch: 21 Cost: 0.1345416298576378\n",
      "Epoch: 22 Cost: 0.13013896662741917\n",
      "Epoch: 23 Cost: 0.12604238837618725\n",
      "Epoch: 24 Cost: 0.12211634944108385\n",
      "Test Accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('log/multilayer_perceptron2', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0\n",
    "        for batch in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([update, loss], feed_dict={X: batch_X,\n",
    "                                                       Y: batch_Y})\n",
    "            average_cost += c / total_batch\n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                      Y: batch_Y})\n",
    "            global_step = epoch*total_batch + batch\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "    \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(X_dict):\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    X = X_dict['images']\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(X, num_hidden_1, tf.nn.relu, name='fc1')\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, num_hidden_2, tf.nn.relu, name='fc2')\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    logits = tf.layers.dense(layer_2, num_classes, name='out')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    logits = multilayer_perceptron(features)\n",
    "\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                 logits=logits, labels=labels),name='loss')\n",
    "        accuracy = tf.metrics.accuracy(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "\n",
    "    with tf.name_scope('optimizer'):\n",
    "        learning_rate = 0.01\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        update = optimizer.minimize(loss,\n",
    "                                    global_step=tf.train.get_global_step())\n",
    "\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.histogram('histogram-loss', loss)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss,\n",
    "        train_op=update,\n",
    "        eval_metric_ops={'accuracy': accuracy})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x181c97af98>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3240983, step = 1\n",
      "INFO:tensorflow:global_step/sec: 239.482\n",
      "INFO:tensorflow:loss = 1.7728766, step = 101 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.212\n",
      "INFO:tensorflow:loss = 1.2416974, step = 201 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.785\n",
      "INFO:tensorflow:loss = 0.90739834, step = 301 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.656\n",
      "INFO:tensorflow:loss = 0.633636, step = 401 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.722\n",
      "INFO:tensorflow:loss = 0.6099938, step = 501 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.691\n",
      "INFO:tensorflow:loss = 0.6928723, step = 601 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.455\n",
      "INFO:tensorflow:loss = 0.47528398, step = 701 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.692\n",
      "INFO:tensorflow:loss = 0.32653236, step = 801 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.847\n",
      "INFO:tensorflow:loss = 0.4268006, step = 901 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.776\n",
      "INFO:tensorflow:loss = 0.2887102, step = 1001 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.474\n",
      "INFO:tensorflow:loss = 0.36707115, step = 1101 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.104\n",
      "INFO:tensorflow:loss = 0.37811956, step = 1201 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.205\n",
      "INFO:tensorflow:loss = 0.24650377, step = 1301 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.923\n",
      "INFO:tensorflow:loss = 0.32238364, step = 1401 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.548\n",
      "INFO:tensorflow:loss = 0.37275034, step = 1501 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.376\n",
      "INFO:tensorflow:loss = 0.3773331, step = 1601 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.329\n",
      "INFO:tensorflow:loss = 0.3100102, step = 1701 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.334\n",
      "INFO:tensorflow:loss = 0.2926942, step = 1801 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.309\n",
      "INFO:tensorflow:loss = 0.22825822, step = 1901 (0.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.936\n",
      "INFO:tensorflow:loss = 0.38949537, step = 2001 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.187\n",
      "INFO:tensorflow:loss = 0.11541493, step = 2101 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.656\n",
      "INFO:tensorflow:loss = 0.35640004, step = 2201 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.717\n",
      "INFO:tensorflow:loss = 0.25524706, step = 2301 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.046\n",
      "INFO:tensorflow:loss = 0.2732802, step = 2401 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.345\n",
      "INFO:tensorflow:loss = 0.23652542, step = 2501 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.786\n",
      "INFO:tensorflow:loss = 0.15657504, step = 2601 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.827\n",
      "INFO:tensorflow:loss = 0.17799367, step = 2701 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.917\n",
      "INFO:tensorflow:loss = 0.25340196, step = 2801 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.818\n",
      "INFO:tensorflow:loss = 0.18117118, step = 2901 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.274\n",
      "INFO:tensorflow:loss = 0.40909755, step = 3001 (0.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.199\n",
      "INFO:tensorflow:loss = 0.31386548, step = 3101 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.449\n",
      "INFO:tensorflow:loss = 0.19484442, step = 3201 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.776\n",
      "INFO:tensorflow:loss = 0.2726854, step = 3301 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.91\n",
      "INFO:tensorflow:loss = 0.19425893, step = 3401 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.069\n",
      "INFO:tensorflow:loss = 0.3121909, step = 3501 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.823\n",
      "INFO:tensorflow:loss = 0.2365881, step = 3601 (0.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.806\n",
      "INFO:tensorflow:loss = 0.31625783, step = 3701 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.013\n",
      "INFO:tensorflow:loss = 0.11293273, step = 3801 (0.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.159\n",
      "INFO:tensorflow:loss = 0.26701343, step = 3901 (0.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.711\n",
      "INFO:tensorflow:loss = 0.24090382, step = 4001 (0.857 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.682\n",
      "INFO:tensorflow:loss = 0.17926148, step = 4101 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.883\n",
      "INFO:tensorflow:loss = 0.2478786, step = 4201 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.365\n",
      "INFO:tensorflow:loss = 0.14586964, step = 4301 (0.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.918\n",
      "INFO:tensorflow:loss = 0.35951608, step = 4401 (0.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.771\n",
      "INFO:tensorflow:loss = 0.15912426, step = 4501 (0.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.611\n",
      "INFO:tensorflow:loss = 0.32917944, step = 4601 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.258\n",
      "INFO:tensorflow:loss = 0.18930726, step = 4701 (0.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.941\n",
      "INFO:tensorflow:loss = 0.24800149, step = 4801 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.536\n",
      "INFO:tensorflow:loss = 0.1476735, step = 4901 (0.521 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.17756143.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-09-00:16:52\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3/model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-09-00:16:52\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9386, global_step = 5000, loss = 0.2187681\n",
      "Testing Accuracy: 0.9386\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "batch_size = 128\n",
    "display_step = 500\n",
    "\n",
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)\n",
    "\n",
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "e = model.evaluate(input_fn)\n",
    "\n",
    "print(\"Testing Accuracy:\", e['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Improve the above network by even a bit.\n",
    "For example, you can try CNN, dropout, different activation function, tune the learning rate, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
