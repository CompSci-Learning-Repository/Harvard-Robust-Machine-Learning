{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Classification, Regression\n",
    "This section will cover logistic regression (which is a classification model) and linear regression. I first demonstrate how to train a logistic regression using the Iris dataset. As an exercise, you will train a linear regression on the Boston housing dataset.\n",
    "\n",
    "This section will put together what we've learned so far. This section might seem hard at first, but the only new concept that will be introduced here is the `GradientDescentOptimizer`. As long as you understand the code step by step, it shouldn't be too hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This example is adopted from [Deep MNIST for Experts](https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/) tutorial from TensorFlow official website.\n",
    "\n",
    "### Task\n",
    "\n",
    "Classify a dataset of hand-written digits (0 through 9).\n",
    "\n",
    "### Data\n",
    "\n",
    "Let's load the data first. Thankfully, TensorFlow provided us a nice API. The data is also already split into train, validation and test set. In general, the data is way nastier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first image to get a sense of what's inside. Note that `next_batch` is also an API provided by TensorFlow which you generally have to code up yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34509805, 0.8313726 , 0.9960785 , 1.        , 0.9960785 ,\n",
       "        0.54901963, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.17254902, 0.78823537, 0.9921569 , 0.9960785 ,\n",
       "        0.9490197 , 0.6901961 , 0.8313726 , 0.9921569 , 0.3372549 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.07058824, 0.8000001 ,\n",
       "        0.9960785 , 0.8431373 , 0.3647059 , 0.02745098, 0.        ,\n",
       "        0.05490196, 0.7607844 , 0.57254905, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.6509804 , 0.9960785 , 0.77647066, 0.07058824,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34901962,\n",
       "        0.8941177 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.15294118, 0.9607844 ,\n",
       "        0.86274517, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.41960788, 0.9607844 , 0.15294118,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3372549 , 0.9960785 , 0.4666667 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5176471 , 0.9960785 , 0.56078434, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.2901961 ,\n",
       "        0.9960785 , 0.43921572, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02745098, 0.82745105, 0.9960785 ,\n",
       "        0.5137255 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.20392159, 0.97647065, 0.6666667 ,\n",
       "        0.09411766, 0.07058824, 0.        , 0.        , 0.09803922,\n",
       "        0.6431373 , 0.9960785 , 0.9490197 , 0.11764707, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.78823537, 0.9960785 , 0.9215687 , 0.9058824 ,\n",
       "        0.6901961 , 0.6901961 , 0.92549026, 0.9960785 , 0.9960785 ,\n",
       "        0.6156863 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.03921569,\n",
       "        0.6431373 , 0.8235295 , 0.9725491 , 0.9960785 , 0.9686275 ,\n",
       "        0.8235295 , 0.9607844 , 0.9960785 , 0.57254905, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.14509805, 0.16862746, 0.14117648, 0.02352941, 0.8235295 ,\n",
       "        0.9960785 , 0.29411766, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.4156863 , 0.9960785 , 0.909804  , 0.04313726,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.627451  ,\n",
       "        0.9960785 , 0.7372549 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.8117648 , 0.9960785 , 0.5529412 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.06666667,\n",
       "        0.89019614, 0.9843138 , 0.39607847, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764707, 0.        ,\n",
       "        0.        , 0.        , 0.40784317, 0.9960785 , 0.8745099 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.49411768, 0.9960785 , 0.627451  , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.49411768, 0.9960785 ,\n",
       "        0.5372549 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.7254902 , 0.9960785 , 0.30980393, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.7137255 ,\n",
       "        0.9960785 , 0.30980393, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = mnist.train.next_batch(1)\n",
    "plotData = batch[0]\n",
    "plotData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $28\\times 28=784$ pixels. $0$ means black and $1$ means white. Let's reshape it and use matplotlib to make it an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3754d4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotData = plotData.reshape(28, 28)\n",
    "plt.gray() # use this line if you don't want to see it in color\n",
    "plt.imshow(plotData)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a softmax regression. Softmax regression is just a multi-class generalization of logistic regression model:\n",
    "\n",
    "$Y=softmax(WX+b)$\n",
    "where\n",
    "\n",
    "$Y: N \\times K$\n",
    "\n",
    "$X: N \\times M$ \n",
    "\n",
    "$W: M \\times N$\n",
    "\n",
    "$b: K$\n",
    "\n",
    "Here, $N$ can either be the train, validation, test sample size or the mini-batch size, depending on which part of the process you're in. $M$ is the number of features (784 in this case) and $K$ is the number of classes (10 in this case).\n",
    "\n",
    "I really like to write down all the matrix dimensions before I start any coding. This way, I can avoid matrix dimension errors later on.\n",
    "\n",
    "The loss we would like to optimize is the cross-entropy loss, averaged over the sample size:\n",
    "\n",
    "$$\\frac{-\\sum_{n=1}^N \\sum_{m=1}^M y_{n,m} log(\\hat{y}_{n,m})}{N}$$\n",
    "\n",
    "We will train this using gradient descent. \n",
    "\n",
    "Coding of this model is a six step process:\n",
    "1. Define variables and placeholders.\n",
    "2. Define the model.\n",
    "3. Define the loss function.\n",
    "4. Define the optimizer.\n",
    "5. Train the model, i.e. initialize variables and run optimizer.\n",
    "6. Evaluate the model.\n",
    "\n",
    "This is the same for any other models. Neural nets will have this form too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y,X$ will be a placeholder, because we will feed in different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 784\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, M])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W,b$ will be variables, because we would like to mutate them every epoch of the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([M, K]))\n",
    "b = tf.Variable(tf.zeros([K]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Softmax regression model is a one liner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "The cross-entropy loss is also a one liner! Make sure to compare with the equation and check that it's doing what it's supposed to. We will also define accuracy for evaluating the result on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss               = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(Yhat), axis=1))\n",
    "correct_prediction = tf.equal(tf.argmax(Yhat, 1), tf.argmax(Y, 1))\n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Here's somethig new to learn! `GradientDescentOptimizer` takes in the objective that we would like to minimize. Running this optimizer will look at all trainable variables that loss depends on and update them. There are many other optimizers e.g. `AdagradOptimizer`, `AdamOptimizer`. Find your favorite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables and run optimizer, evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 1.1834820735454579\n",
      "Epoch: 1 Cost: 0.6654252042553639\n",
      "Epoch: 2 Cost: 0.5528633421659467\n",
      "Epoch: 3 Cost: 0.4985371092774651\n",
      "Epoch: 4 Cost: 0.46536690576509965\n",
      "Epoch: 5 Cost: 0.44257826485417173\n",
      "Epoch: 6 Cost: 0.4256648432395674\n",
      "Epoch: 7 Cost: 0.41208646367896673\n",
      "Epoch: 8 Cost: 0.4012913585521958\n",
      "Epoch: 9 Cost: 0.3922749308293518\n",
      "Epoch: 10 Cost: 0.3849485085769133\n",
      "Epoch: 11 Cost: 0.37807464090260584\n",
      "Epoch: 12 Cost: 0.37228546719659444\n",
      "Epoch: 13 Cost: 0.36736478854309407\n",
      "Epoch: 14 Cost: 0.36301442536440753\n",
      "Epoch: 15 Cost: 0.35820750816301883\n",
      "Epoch: 16 Cost: 0.35497923772443385\n",
      "Epoch: 17 Cost: 0.3511378937146877\n",
      "Epoch: 18 Cost: 0.34873683661222443\n",
      "Epoch: 19 Cost: 0.34532226362011625\n",
      "Epoch: 20 Cost: 0.3427596790411255\n",
      "Epoch: 21 Cost: 0.33980026437477623\n",
      "Epoch: 22 Cost: 0.33851496119390817\n",
      "Epoch: 23 Cost: 0.33536126572977426\n",
      "Epoch: 24 Cost: 0.3342185702378103\n",
      "Train Accuracy: 0.90745455\n",
      "Test Accuracy: 0.9142\n"
     ]
    }
   ],
   "source": [
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0\n",
    "        for _ in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, loss], feed_dict={X: batch_X,\n",
    "                                                          Y: batch_Y})\n",
    "            average_cost += c / total_batch\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "        \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Train a linear regression using gradient descent (no closed form!) that predicts housing prices in Boston from other covariates provided. Follow the six steps above. Plot the loss and observe that it goes down.\n",
    "\n",
    "(Optional) Fit a LASSO. Fit a Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data)\n",
    "df.columns = boston.feature_names\n",
    "df['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
