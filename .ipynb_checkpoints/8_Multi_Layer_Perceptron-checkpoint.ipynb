{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "We introduce a 3-layer perceptron, i.e. a 3 layer fully connected neural network. We do this using the MNIST data once again. We first write the model as we were doing before. Later, we will show how we can cleanly organize the functions to a Python `Class`. It's time to get object oriented :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = mnist.train.images.shape[1]\n",
    "num_classes  = mnist.train.labels.shape[1]\n",
    "num_hidden_1 = 256\n",
    "num_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, num_features], name='Input-Images')\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, num_classes], name='Output-Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('fc1'):\n",
    "    W1 = tf.Variable(tf.random_normal([num_features, num_hidden_1]),name='weights')\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden_1]),name='bias')\n",
    "\n",
    "with tf.name_scope('fc2'):\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2]),name='weights')\n",
    "    b2 = tf.Variable(tf.random_normal([num_hidden_2]),name='bias')\n",
    "\n",
    "with tf.name_scope('out'):\n",
    "    Wout = tf.Variable(tf.random_normal([num_hidden_2, num_classes]),name='weights')\n",
    "    bout = tf.Variable(tf.random_normal([num_classes]),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('multilayer_perceptron'):\n",
    "    H1 = tf.nn.relu(X @ W1 + b1, name='H1')\n",
    "    H2 = tf.nn.relu(H1 @ W2 + b2, name='H2')\n",
    "    logits = tf.add(H2 @ Wout, bout, name='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                             logits=logits, labels=Y),name='loss')\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 61.8643349510972\n",
      "Epoch: 1 Cost: 12.187720913453521\n",
      "Epoch: 2 Cost: 7.424538441219112\n",
      "Epoch: 3 Cost: 5.4271530052884\n",
      "Epoch: 4 Cost: 3.982053814406861\n",
      "Epoch: 5 Cost: 3.151202641244398\n",
      "Epoch: 6 Cost: 2.6753018941547957\n",
      "Epoch: 7 Cost: 2.1888300399692033\n",
      "Epoch: 8 Cost: 1.813328645066876\n",
      "Epoch: 9 Cost: 1.4697528642732964\n",
      "Epoch: 10 Cost: 1.3372768463187745\n",
      "Epoch: 11 Cost: 1.2398637867008184\n",
      "Epoch: 12 Cost: 0.9818570881490868\n",
      "Epoch: 13 Cost: 0.8850345878592402\n",
      "Epoch: 14 Cost: 0.7661460467218224\n",
      "Epoch: 15 Cost: 0.7395797746256713\n",
      "Epoch: 16 Cost: 0.5951993956270213\n",
      "Epoch: 17 Cost: 0.5810083896863325\n",
      "Epoch: 18 Cost: 0.4626998142226008\n",
      "Epoch: 19 Cost: 0.44251203135753675\n",
      "Epoch: 20 Cost: 0.4166008296509719\n",
      "Epoch: 21 Cost: 0.35637803889022446\n",
      "Epoch: 22 Cost: 0.30885852929234\n",
      "Epoch: 23 Cost: 0.29982214903542986\n",
      "Epoch: 24 Cost: 0.2562230518264089\n",
      "Test Accuracy: 0.9276\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('log/multilayer_perceptron1', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0\n",
    "        for batch in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([update, loss], feed_dict={X: batch_X,\n",
    "                                                       Y: batch_Y})\n",
    "            average_cost += c / total_batch\n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                      Y: batch_Y})\n",
    "            global_step = epoch*total_batch + batch\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "    \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, num_features], name='Input-Images')\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, num_classes], name='Output-Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('multilayer_perceptron'):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(X, num_hidden_1, tf.nn.relu, name='fc1')\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, num_hidden_2, tf.nn.relu, name='fc2')\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    logits = tf.layers.dense(layer_2, num_classes, name='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                             logits=logits, labels=Y),name='loss')\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 1.1429620449651363\n",
      "Epoch: 1 Cost: 0.44573915083299925\n",
      "Epoch: 2 Cost: 0.35664164621721617\n",
      "Epoch: 3 Cost: 0.31739128164269714\n",
      "Epoch: 4 Cost: 0.2910888133265754\n",
      "Epoch: 5 Cost: 0.2713912793858482\n",
      "Epoch: 6 Cost: 0.25498610958456974\n",
      "Epoch: 7 Cost: 0.2407954240658066\n",
      "Epoch: 8 Cost: 0.22825071882117884\n",
      "Epoch: 9 Cost: 0.21688424812121837\n",
      "Epoch: 10 Cost: 0.20677164849909882\n",
      "Epoch: 11 Cost: 0.19733745005320408\n",
      "Epoch: 12 Cost: 0.1889122671769422\n",
      "Epoch: 13 Cost: 0.18079517665234468\n",
      "Epoch: 14 Cost: 0.17358795196495266\n",
      "Epoch: 15 Cost: 0.16686928619037983\n",
      "Epoch: 16 Cost: 0.1604892960935832\n",
      "Epoch: 17 Cost: 0.15470593801953564\n",
      "Epoch: 18 Cost: 0.14925623104653593\n",
      "Epoch: 19 Cost: 0.14406697281382297\n",
      "Epoch: 20 Cost: 0.1391177260740237\n",
      "Epoch: 21 Cost: 0.1345416298576378\n",
      "Epoch: 22 Cost: 0.13013896662741917\n",
      "Epoch: 23 Cost: 0.12604238837618725\n",
      "Epoch: 24 Cost: 0.12211634944108385\n",
      "Test Accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('log/multilayer_perceptron2', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0\n",
    "        for batch in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([update, loss], feed_dict={X: batch_X,\n",
    "                                                       Y: batch_Y})\n",
    "            average_cost += c / total_batch\n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                      Y: batch_Y})\n",
    "            global_step = epoch*total_batch + batch\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "    \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(X_dict):\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    X = X_dict['images']\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(X, num_hidden_1, tf.nn.relu, name='fc1')\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, num_hidden_2, tf.nn.relu, name='fc2')\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    logits = tf.layers.dense(layer_2, num_classes, name='out')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    logits = multilayer_perceptron(features)\n",
    "\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits)\n",
    "\n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                 logits=logits, labels=labels),name='loss')\n",
    "        accuracy = tf.metrics.accuracy(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "\n",
    "    with tf.name_scope('optimizer'):\n",
    "        learning_rate = 0.01\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        update = optimizer.minimize(loss,\n",
    "                                    global_step=tf.train.get_global_step())\n",
    "\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.histogram('histogram-loss', loss)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss,\n",
    "        train_op=update,\n",
    "        eval_metric_ops={'accuracy': accuracy})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpf30g5v3w\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpf30g5v3w', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x181602a1d0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpf30g5v3w/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3746989, step = 1\n",
      "INFO:tensorflow:global_step/sec: 232.79\n",
      "INFO:tensorflow:loss = 1.6682366, step = 101 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 162.459\n",
      "INFO:tensorflow:loss = 1.1323845, step = 201 (0.617 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.951\n",
      "INFO:tensorflow:loss = 0.846112, step = 301 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.759\n",
      "INFO:tensorflow:loss = 0.62150025, step = 401 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.795\n",
      "INFO:tensorflow:loss = 0.56862587, step = 501 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.752\n",
      "INFO:tensorflow:loss = 0.471189, step = 601 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.649\n",
      "INFO:tensorflow:loss = 0.48884785, step = 701 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.194\n",
      "INFO:tensorflow:loss = 0.36440235, step = 801 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.374\n",
      "INFO:tensorflow:loss = 0.45894647, step = 901 (0.425 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpf30g5v3w/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.3654167.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-09-00:15:18\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpf30g5v3w/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-09-00:15:18\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9008, global_step = 1000, loss = 0.36456984\n",
      "Testing Accuracy: 0.9008\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn)\n",
    "\n",
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)\n",
    "\n",
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "e = model.evaluate(input_fn)\n",
    "\n",
    "print(\"Testing Accuracy:\", e['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "If you're interested in even cleaner code, check out [Estimators](https://www.tensorflow.org/get_started/custom_estimators). I didn't introduce it here since it's still being developed, it can be a standard way to develop deep learning models in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
