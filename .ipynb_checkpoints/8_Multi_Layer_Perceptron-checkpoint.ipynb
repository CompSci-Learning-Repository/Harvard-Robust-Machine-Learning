{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "We introduce a multi-layer perceptron, aka a 3 layer fully connected neural network. We do this using the MNIST data once again. We first write the model as we were doing before. Next, we will clean it up a bit using more TensorFlow API. Finally, we will show how we can cleanly organize the functions using `tf.Estimaor`.\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = mnist.train.images.shape[1]\n",
    "num_classes  = mnist.train.labels.shape[1]\n",
    "num_hidden_1 = 256\n",
    "num_hidden_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a three layer fully connected neural network. It's actually very straightforward.\n",
    "We have two hidden layers and one output layer. Two hidden layers have a activation function (chosen to be ReLU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, num_features], name='Input-Images')\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, num_classes], name='Output-Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('fc1'): # first hidden layer variables\n",
    "    W1 = tf.Variable(tf.random_normal([num_features, num_hidden_1]),name='weights')\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden_1]),name='bias')\n",
    "\n",
    "with tf.name_scope('fc2'): # second hidden layer variables\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2]),name='weights')\n",
    "    b2 = tf.Variable(tf.random_normal([num_hidden_2]),name='bias')\n",
    "\n",
    "with tf.name_scope('out'): # output layer variables\n",
    "    Wout = tf.Variable(tf.random_normal([num_hidden_2, num_classes]),name='weights')\n",
    "    bout = tf.Variable(tf.random_normal([num_classes]),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('multilayer_perceptron'): # 3 layer fully connected network\n",
    "    H1 = tf.nn.relu(X @ W1 + b1, name='H1')\n",
    "    H2 = tf.nn.relu(H1 @ W2 + b2, name='H2')\n",
    "    logits = tf.add(H2 @ Wout, bout, name='out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final layer outputs a `logit`, which is just the output of the neural network before `softmax` is applied to make it a probability. This is because TensorFlow has a convenient API `tf.nn.softmax_cross_entropy_with_logits` that applies `softmax` to `logit`, and then computes a `cross_entopy` on each element. So, all we have to do is to sum up those cross entropies to reduce it to a single loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                             logits=logits, labels=Y),name='loss')\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the model just like we did before. The accuracy doesn't seem to improve much (with more epochs, it should)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 63.5227008659\n",
      "Epoch: 1 Cost: 11.7255829098\n",
      "Epoch: 2 Cost: 7.04268976911\n",
      "Epoch: 3 Cost: 4.94169215168\n",
      "Epoch: 4 Cost: 3.7582281366\n",
      "Epoch: 5 Cost: 3.02995569802\n",
      "Epoch: 6 Cost: 2.51047183418\n",
      "Epoch: 7 Cost: 2.11883776171\n",
      "Epoch: 8 Cost: 1.82096150981\n",
      "Epoch: 9 Cost: 1.56255665463\n",
      "Epoch: 10 Cost: 1.36567232466\n",
      "Epoch: 11 Cost: 1.20529388641\n",
      "Epoch: 12 Cost: 1.07357674758\n",
      "Epoch: 13 Cost: 0.954112300066\n",
      "Epoch: 14 Cost: 0.868395346037\n",
      "Epoch: 15 Cost: 0.793990999275\n",
      "Epoch: 16 Cost: 0.716135683926\n",
      "Epoch: 17 Cost: 0.652914618337\n",
      "Epoch: 18 Cost: 0.601221448492\n",
      "Epoch: 19 Cost: 0.552582887719\n",
      "Epoch: 20 Cost: 0.508905344259\n",
      "Epoch: 21 Cost: 0.476214547663\n",
      "Epoch: 22 Cost: 0.442873191204\n",
      "Epoch: 23 Cost: 0.410599114106\n",
      "Epoch: 24 Cost: 0.384783977855\n",
      "Test Accuracy: 0.9184\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('log/multilayer_perceptron1', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0\n",
    "        for batch in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([update, loss], feed_dict={X: batch_X,\n",
    "                                                       Y: batch_Y})\n",
    "            average_cost += c / total_batch\n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                      Y: batch_Y})\n",
    "            global_step = epoch*total_batch + batch\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "    \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only differense in this version is that each layer is replaced with a single API `tf.layers.dense`. The convenient thing about this is that 1) it's clean and 2) you don't need to initialize weight and bias variables yourself. The rest is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(tf.float32, shape=[None, num_features], name='Input-Images')\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, num_classes], name='Output-Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('multilayer_perceptron'): # Now, it is only three lines\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.layers.dense(X, num_hidden_1, tf.nn.relu, name='fc1')\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.layers.dense(layer_1, num_hidden_2, tf.nn.relu, name='fc2')\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    logits = tf.layers.dense(layer_2, num_classes, name='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                             logits=logits, labels=Y),name='loss')\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    learning_rate = 0.01\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 1.1429620449651363\n",
      "Epoch: 1 Cost: 0.44573915083299925\n",
      "Epoch: 2 Cost: 0.35664164621721617\n",
      "Epoch: 3 Cost: 0.31739128164269714\n",
      "Epoch: 4 Cost: 0.2910888133265754\n",
      "Epoch: 5 Cost: 0.2713912793858482\n",
      "Epoch: 6 Cost: 0.25498610958456974\n",
      "Epoch: 7 Cost: 0.2407954240658066\n",
      "Epoch: 8 Cost: 0.22825071882117884\n",
      "Epoch: 9 Cost: 0.21688424812121837\n",
      "Epoch: 10 Cost: 0.20677164849909882\n",
      "Epoch: 11 Cost: 0.19733745005320408\n",
      "Epoch: 12 Cost: 0.1889122671769422\n",
      "Epoch: 13 Cost: 0.18079517665234468\n",
      "Epoch: 14 Cost: 0.17358795196495266\n",
      "Epoch: 15 Cost: 0.16686928619037983\n",
      "Epoch: 16 Cost: 0.1604892960935832\n",
      "Epoch: 17 Cost: 0.15470593801953564\n",
      "Epoch: 18 Cost: 0.14925623104653593\n",
      "Epoch: 19 Cost: 0.14406697281382297\n",
      "Epoch: 20 Cost: 0.1391177260740237\n",
      "Epoch: 21 Cost: 0.1345416298576378\n",
      "Epoch: 22 Cost: 0.13013896662741917\n",
      "Epoch: 23 Cost: 0.12604238837618725\n",
      "Epoch: 24 Cost: 0.12211634944108385\n",
      "Test Accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('log/multilayer_perceptron2', sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0\n",
    "        for batch in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([update, loss], feed_dict={X: batch_X,\n",
    "                                                       Y: batch_Y})\n",
    "            average_cost += c / total_batch\n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                      Y: batch_Y})\n",
    "            global_step = epoch*total_batch + batch\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "    \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is much better now! Why? Probably because the weight initialization method is different. When we manually initialize the network variables, we use the standard normal distribution. Probably Tensorflow uses random normal but with smaller variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final version introduces the newly introduced `tf.estimator`. This is essentially a wrapper for the models you create. If we use `tf.estimator` we no longer have to code up `for` loops for the mini batch training. All we need to do is to write the model, and tell the estimator to run it for a certain number of steps. `tf.estimator` is also cool because it is very easy to make a prediction, and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Clearing all tensors before this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a multi-layer perceptron just like we did before, but this time, let's make it a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(X_dict):\n",
    "    # Estimator input is a dict, in case of multiple inputs\n",
    "    X = X_dict['images']\n",
    "    layer_1 = tf.layers.dense(X, num_hidden_1, tf.nn.relu, name='fc1')\n",
    "    layer_2 = tf.layers.dense(layer_1, num_hidden_2, tf.nn.relu, name='fc2')\n",
    "    logits = tf.layers.dense(layer_2, num_classes, name='out')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the input to the estimator. We need to specify the entire model, including the loss, optimizer, etc. We build a function `model_fn` with a pre-specified signature.\n",
    "\n",
    "`model_fn` should take in features, labels and mode. Mode tells the function whether you're running it in `TRAIN`, `EVAL` or `PREDICTION`. You can code different behaviors for each mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    # get logit prediction from NN\n",
    "    logits = multilayer_perceptron(features)\n",
    "\n",
    "    pred_classes = tf.argmax(logits, axis=1)\n",
    "\n",
    "    # if prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "    \n",
    "    # these are all the same code as before\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                                 logits=logits, labels=labels),name='loss')\n",
    "        accuracy = tf.metrics.accuracy(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "\n",
    "    with tf.name_scope('optimizer'):\n",
    "        learning_rate = 0.01\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        update = optimizer.minimize(loss,\n",
    "                                    global_step=tf.train.get_global_step())\n",
    "\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.histogram('histogram-loss', loss)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "    # Estimator requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=pred_classes,\n",
    "        loss=loss,\n",
    "        train_op=update,\n",
    "        eval_metric_ops={'accuracy': accuracy})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can build the model with a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.Estimator(model_fn) # build the Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training then becomes extremely simple. First, define an input using `tf.estimator.inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input function for training\n",
    "batch_size = 128\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.train.images}, y=mnist.train.labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, training is a one liner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x181c97af98>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3240983, step = 1\n",
      "INFO:tensorflow:global_step/sec: 239.482\n",
      "INFO:tensorflow:loss = 1.7728766, step = 101 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.212\n",
      "INFO:tensorflow:loss = 1.2416974, step = 201 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.785\n",
      "INFO:tensorflow:loss = 0.90739834, step = 301 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.656\n",
      "INFO:tensorflow:loss = 0.633636, step = 401 (0.494 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.722\n",
      "INFO:tensorflow:loss = 0.6099938, step = 501 (0.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.691\n",
      "INFO:tensorflow:loss = 0.6928723, step = 601 (0.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.455\n",
      "INFO:tensorflow:loss = 0.47528398, step = 701 (0.517 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.692\n",
      "INFO:tensorflow:loss = 0.32653236, step = 801 (0.479 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.847\n",
      "INFO:tensorflow:loss = 0.4268006, step = 901 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.776\n",
      "INFO:tensorflow:loss = 0.2887102, step = 1001 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.474\n",
      "INFO:tensorflow:loss = 0.36707115, step = 1101 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.104\n",
      "INFO:tensorflow:loss = 0.37811956, step = 1201 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 223.205\n",
      "INFO:tensorflow:loss = 0.24650377, step = 1301 (0.448 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.923\n",
      "INFO:tensorflow:loss = 0.32238364, step = 1401 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 197.548\n",
      "INFO:tensorflow:loss = 0.37275034, step = 1501 (0.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 128.376\n",
      "INFO:tensorflow:loss = 0.3773331, step = 1601 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.329\n",
      "INFO:tensorflow:loss = 0.3100102, step = 1701 (0.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 179.334\n",
      "INFO:tensorflow:loss = 0.2926942, step = 1801 (0.560 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.309\n",
      "INFO:tensorflow:loss = 0.22825822, step = 1901 (0.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.936\n",
      "INFO:tensorflow:loss = 0.38949537, step = 2001 (0.477 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.187\n",
      "INFO:tensorflow:loss = 0.11541493, step = 2101 (0.467 sec)\n",
      "INFO:tensorflow:global_step/sec: 181.656\n",
      "INFO:tensorflow:loss = 0.35640004, step = 2201 (0.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.717\n",
      "INFO:tensorflow:loss = 0.25524706, step = 2301 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.046\n",
      "INFO:tensorflow:loss = 0.2732802, step = 2401 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 185.345\n",
      "INFO:tensorflow:loss = 0.23652542, step = 2501 (0.536 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.786\n",
      "INFO:tensorflow:loss = 0.15657504, step = 2601 (0.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.827\n",
      "INFO:tensorflow:loss = 0.17799367, step = 2701 (0.453 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.917\n",
      "INFO:tensorflow:loss = 0.25340196, step = 2801 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 182.818\n",
      "INFO:tensorflow:loss = 0.18117118, step = 2901 (0.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.274\n",
      "INFO:tensorflow:loss = 0.40909755, step = 3001 (0.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 203.199\n",
      "INFO:tensorflow:loss = 0.31386548, step = 3101 (0.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 180.449\n",
      "INFO:tensorflow:loss = 0.19484442, step = 3201 (0.558 sec)\n",
      "INFO:tensorflow:global_step/sec: 205.776\n",
      "INFO:tensorflow:loss = 0.2726854, step = 3301 (0.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 198.91\n",
      "INFO:tensorflow:loss = 0.19425893, step = 3401 (0.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.069\n",
      "INFO:tensorflow:loss = 0.3121909, step = 3501 (0.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.823\n",
      "INFO:tensorflow:loss = 0.2365881, step = 3601 (0.791 sec)\n",
      "INFO:tensorflow:global_step/sec: 125.806\n",
      "INFO:tensorflow:loss = 0.31625783, step = 3701 (0.794 sec)\n",
      "INFO:tensorflow:global_step/sec: 124.013\n",
      "INFO:tensorflow:loss = 0.11293273, step = 3801 (0.806 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.159\n",
      "INFO:tensorflow:loss = 0.26701343, step = 3901 (0.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.711\n",
      "INFO:tensorflow:loss = 0.24090382, step = 4001 (0.857 sec)\n",
      "INFO:tensorflow:global_step/sec: 217.682\n",
      "INFO:tensorflow:loss = 0.17926148, step = 4101 (0.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.883\n",
      "INFO:tensorflow:loss = 0.2478786, step = 4201 (0.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.365\n",
      "INFO:tensorflow:loss = 0.14586964, step = 4301 (0.876 sec)\n",
      "INFO:tensorflow:global_step/sec: 136.918\n",
      "INFO:tensorflow:loss = 0.35951608, step = 4401 (0.729 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.771\n",
      "INFO:tensorflow:loss = 0.15912426, step = 4501 (0.573 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.611\n",
      "INFO:tensorflow:loss = 0.32917944, step = 4601 (0.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.258\n",
      "INFO:tensorflow:loss = 0.18930726, step = 4701 (0.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.941\n",
      "INFO:tensorflow:loss = 0.24800149, step = 4801 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 192.536\n",
      "INFO:tensorflow:loss = 0.1476735, step = 4901 (0.521 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.17756143.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-09-00:16:52\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/lj/p0jqksf54pldc98grzy8m6p00000gn/T/tmpv7e4ojg3/model.ckpt-5000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-09-00:16:52\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 0.9386, global_step = 5000, loss = 0.2187681\n",
      "Testing Accuracy: 0.9386\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5000\n",
    "model.train(input_fn, steps=num_steps) # train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, you need to specify the test input and run `.evaluate` instead of `.train`. Then, the mode will be set to `PREDICTION`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "# use the Estimator 'evaluate' method\n",
    "e = model.evaluate(input_fn)\n",
    "\n",
    "print(\"Testing Accuracy:\", e['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You can choose whichever version you prefer when constructing your model. Although we didn't introduce here, it is also a good strategy to create a python `Class` for each of your model and try to be object oriented. In the sense that it tries to \"package\" a model, `Class` is similar to `tf.Estimator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Improve the above network by even a bit.\n",
    "For example, you can try CNN, dropout, different activation function, tune the learning rate, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
