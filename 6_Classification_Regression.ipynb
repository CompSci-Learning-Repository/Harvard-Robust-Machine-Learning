{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Classification, Regression\n",
    "This section will cover logistic regression (which is a classification model, not regression) and linear regression. Once you can train both of these models, you know how to do both classification and regression in their simplest way possible. I first demonstrate how to train a logistic regression using the Iris dataset. As an exercise, you will train a linear regression on the Boston housing dataset.\n",
    "\n",
    "This section will put together what we've learned so far, e.g. `session`, `placeholder`, `variable`, etc. This section might seem hard at first, but the only new concept that will be introduced here is the `GradientDescentOptimizer`. As long as you understand the code step by step, it should be ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This example is partially adopted from [Deep MNIST for Experts](https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/) tutorial from TensorFlow official website.\n",
    "\n",
    "### Task\n",
    "\n",
    "Classify a dataset of hand-written digits (0 through 9).\n",
    "\n",
    "### Data\n",
    "\n",
    "Let's load the data first. Thankfully, TensorFlow provided us a nice API. The data is also already split into train, validation and test set. In general, the data is much less clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first image to get a sense of what is inside. Note that `next_batch` is an API provided by TensorFlow to get the next `n` images from the dataset. You generally have to code this up yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.02352941,  0.26274511,\n",
       "         0.26274511,  0.26274511,  0.52549022,  0.69411767,  0.3921569 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.22352943,  0.3921569 ,\n",
       "         0.56078434,  0.83921576,  0.99215692,  0.99215692,  0.99215692,\n",
       "         0.99215692,  0.99215692,  0.92156869,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.79215693,  0.99215692,  0.99215692,  0.99215692,\n",
       "         0.99215692,  0.96470594,  0.94901967,  0.99215692,  0.99215692,\n",
       "         0.57647061,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.80784321,\n",
       "         0.93725497,  0.77647066,  0.54901963,  0.34509805,  0.3019608 ,\n",
       "         0.58823532,  0.99215692,  0.92549026,  0.25098041,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.03137255,  0.16078432,  0.        ,\n",
       "         0.        ,  0.07843138,  0.57254905,  0.99215692,  0.9333334 ,\n",
       "         0.26274511,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.08235294,  0.78039223,\n",
       "         0.99215692,  0.97254908,  0.2392157 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.04705883,\n",
       "         0.34509805,  0.98039222,  0.99215692,  0.99215692,  0.97647065,\n",
       "         0.3137255 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.17647059,  0.67450982,  0.99215692,  0.99215692,\n",
       "         0.99215692,  0.86274517,  0.99215692,  0.81960791,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.75294125,\n",
       "         0.99215692,  0.99215692,  0.95686281,  0.27843139,  0.37647063,\n",
       "         0.99215692,  0.81960791,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.9450981 ,  1.        ,  0.89019614,\n",
       "         0.227451  ,  0.        ,  0.35294119,  0.99607849,  0.57647061,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.50588238,  0.32549021,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.17254902,  0.3019608 ,  0.05490196,  0.        ,  0.        ,\n",
       "         0.65098041,  0.99215692,  0.38823533,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.99607849,  0.94901967,\n",
       "         0.65490198,  0.18823531,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.08627451,  0.86274517,  0.95686281,\n",
       "         0.06666667,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.28627452,  0.77647066,  0.98039222,  0.97254908,\n",
       "         0.8588236 ,  0.15294118,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00784314,\n",
       "         0.66274512,  0.99215692,  0.72156864,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.28627452,  0.58823532,  0.99215692,  0.87450987,\n",
       "         0.44705886,  0.18039216,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.27843139,  0.99215692,  0.91764712,\n",
       "         0.21568629,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.01960784,  0.30980393,  0.76862752,  0.99215692,  0.95686281,\n",
       "         0.64705884,  0.34901962,  0.        ,  0.09411766,  0.51764709,\n",
       "         0.98431379,  0.99215692,  0.49803925,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.03137255,  0.68627453,  0.95686281,  0.99215692,  0.99215692,\n",
       "         0.91372555,  0.9333334 ,  0.99215692,  0.99215692,  0.60392159,\n",
       "         0.01960784,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.07058824,  0.70588237,  0.81960791,  0.82352948,  0.86666673,\n",
       "         0.99215692,  0.82745105,  0.45490199,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.07058824,  0.25882354,  0.01176471,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = mnist.train.next_batch(1) # get one image from the data\n",
    "plotData = batch[0]\n",
    "plotData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $28\\times 28=784$ pixels. $0$ means black and $1$ means white. Let's reshape it and use `matplotlib` to make it an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADftJREFUeJzt3X+IVfeZx/HPE6NIxhIiRiNpXG0JYTcDxs0gIdaSJaZqMBghJvUvS5cd/zDQwgZWAklNSqEp/bErgYKN0hFabWHSZmJCUxmWTIOLZBKCsRptaKzORrRGiTGS1B/P/jFnthMz53vu3HvuPXfmeb9A5t773HPOw00+c86d7znna+4uAPFcU3UDAKpB+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBHVtKzdmZpxOCDSZu1st72toz29mK8zssJm9a2abGlkXgNayes/tN7Mpko5Iuk/SkKTXJa1z94OJZdjzA03Wij3/Yknvuvuf3f1vknZJWt3A+gC0UCPhv1nS8VHPh7LXPsPMus1s0MwGG9gWgJI18ge/sQ4tPndY7+5bJW2VOOwH2kkje/4hSbeMev5FSe831g6AVmkk/K9LutXMFpjZNElfl9RXTlsAmq3uw353v2Rmj0p6RdIUSdvd/Y+ldQagqeoe6qtrY3znB5quJSf5AJi4CD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiq7im6JcnMjkr6SNJlSZfcvauMpgA0X0Phz/yLu58uYT0AWojDfiCoRsPvkn5vZm+YWXcZDQFojUYP+5e4+/tmNlvSHjN7x90HRr8h+6XALwagzZi7l7Mis82Szrv7DxPvKWdjAHK5u9XyvroP+82sw8y+MPJY0tckHah3fQBaq5HD/jmSfmNmI+v5pbv/rpSuADRdaYf9NW2Mw/5JZ9q0acn60qVLm7bte++9N1m//fbbc2svvvhictnnnnuurp7aQdMP+wFMbIQfCIrwA0ERfiAowg8ERfiBoBjqm+TuuuuuZL2zszNZTw2XSdLy5cuT9dtuuy1Zr8qZM2eS9RtvvLFFnZSPoT4ASYQfCIrwA0ERfiAowg8ERfiBoAg/EFQZd+9FG9uyZUuyfuedd7aok8/79NNPk/VLly4l6x0dHXVve9euXXUvO1mw5weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnn+SuvTb9n/jcuXPJ+qFDh5L1vXv3JuuDg4O5tY8//ji57MqVK5P1DRs2JOtnz57NrfX29iaXjYA9PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EVXjffjPbLmmVpFPu3pm9NlPSryTNl3RU0sPunj+o+vd1cd/+Fps+fXqyPn/+/GT9nXfeaWj7s2bNyq09+eSTyWU3btyYrH/44YfJ+tq1a3Nr/f39yWUnsjLv2/9zSSuuem2TpH53v1VSf/YcwARSGH53H5B09fQmqyX1ZI97JD1Ycl8Amqze7/xz3P2EJGU/Z5fXEoBWaPq5/WbWLam72dsBMD717vlPmtlcScp+nsp7o7tvdfcud++qc1sAmqDe8PdJWp89Xi/phXLaAdAqheE3s52S/kfSbWY2ZGb/Kun7ku4zsz9Jui97DmACKRznL3VjjPNPOkXz2Pf19eXWFi9enFz24sWLyfqyZcuS9ddeey1Zn6zKHOcHMAkRfiAowg8ERfiBoAg/EBThB4Li1t3BzZgxI1lftWpVsr5jx45kfcqUKePuaYRZesQqdbkwirHnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGguKR3kuvs7EzWN2/enKyvWbOmxG4+67333kvWFyxYkKwfOHAgWV+4cOG4e5oMuKQXQBLhB4Ii/EBQhB8IivADQRF+ICjCDwTF9fyT3LPPPpusL126tKH1X7hwIVl/6KGHcmtz585NLrtt27a6ekJt2PMDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCF4/xmtl3SKkmn3L0ze22zpH+T9NfsbY+7+8vNahL1W7RoUbJ+zTXp3/9DQ0PJ+iOPPJKs7927N7e2bt265LJF9+0fGBhI1pFWy57/55JWjPH6T9z9juwfwQcmmMLwu/uApDMt6AVACzXynf9RM9tvZtvN7IbSOgLQEvWG/6eSvizpDkknJP0o741m1m1mg2Y2WOe2ADRBXeF395Puftndr0j6maTFifdudfcud++qt0kA5asr/GY2+nKsNZLSt1EF0HZqGerbKekeSbPMbEjSdyTdY2Z3SHJJRyVtaGKPAJqgMPzuPtZgbCUXWj/11FO5tf379yeX7e3tLbudCWHJkiXJetG98V999dVk/dy5c+PuacQDDzyQrBfNKfHWW2/VvW1whh8QFuEHgiL8QFCEHwiK8ANBEX4gqAk1RfeVK1dya+fPn08u+9JLLyXrzzzzTLLOsFJ9Zs+enVtLXe4rSXPmzEnW582bl6yfPXs2WZ+smKIbQBLhB4Ii/EBQhB8IivADQRF+ICjCDwQ1ocb5V65cmVt74oknkst2daVvJJQ6h0CSjh8/nltbtWpVctnDhw8n6xNZ0a2/d+7cmVtLTd8tSXv27EnWV6wY66bSYJwfQBLhB4Ii/EBQhB8IivADQRF+ICjCDwQ1ocb5G5E6R0Aqvo30hg35UxMcO3YsueyWLVuS9f7+/mS96LbkVVq+fHmy/vLL+RM4f/DBB8lli/6b7Nu3L1mPinF+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxBU4Ti/md0iaYekmyRdkbTV3f/LzGZK+pWk+ZKOSnrY3ZM3Sq9ynL/I1KlTk/X7778/t/bYY48ll7377ruT9QsXLiTrfX19yfru3btza6nr6SXppptuStY3btyYrG/atClZT13v//TTTyeXTU3JjnxljvNfkvTv7v6Pku6StNHM/knSJkn97n6rpP7sOYAJojD87n7C3d/MHn8k6ZCkmyWtltSTva1H0oPNahJA+cb1nd/M5ktaJGmfpDnufkIa/gUhKX9eJgBt59pa32hmMyT1Svq2u58zq+lrhcysW1J3fe0BaJaa9vxmNlXDwf+Fuz+fvXzSzOZm9bmSTo21rLtvdfcud0/fQRNASxWG34Z38dskHXL3H48q9Ulanz1eL+mF8tsD0Cy1DPV9RdIfJL2t4aE+SXpcw9/7fy1pnqRjkta6+5mCdbXtUF8jpk+fnqwXDbctW7YsWb/uuuvG3dOI06dPJ+tFQ5zXX3993duWpG3btuXWioYRL1682NC2o6p1qK/wO7+7vyYpb2X3jqcpAO2DM/yAoAg/EBThB4Ii/EBQhB8IivADQdV8ei/yffLJJ8n6mjVrkvWZM2cm6z09Pcn6vHnzcmtFp2F3dHQk60Xj/AcPHkzWjxw5kltjHL9a7PmBoAg/EBThB4Ii/EBQhB8IivADQRF+IKgwU3RjbEXnGCxcuDBZHxgYSNYvX7487p7QGKboBpBE+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc4PTDKM8wNIIvxAUIQfCIrwA0ERfiAowg8ERfiBoArDb2a3mNl/m9khM/ujmX0re32zmf2vmb2V/bu/+e0CKEvhST5mNlfSXHd/08y+IOkNSQ9KeljSeXf/Yc0b4yQfoOlqPcmncMYedz8h6UT2+CMzOyTp5sbaA1C1cX3nN7P5khZJ2pe99KiZ7Tez7WZ2Q84y3WY2aGaDDXUKoFQ1n9tvZjMkvSrpe+7+vJnNkXRakkv6roa/GnyzYB0c9gNNVuthf03hN7OpknZLesXdfzxGfb6k3e7eWbAewg80WWkX9tjwNK/bJB0aHfzsD4Ej1kg6MN4mAVSnlr/2f0XSHyS9LelK9vLjktZJukPDh/1HJW3I/jiYWhd7fqDJSj3sLwvhB5qP6/kBJBF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCKryBZ8lOS/rLqOezstfaUbv21q59SfRWrzJ7+4da39jS6/k/t3GzQXfvqqyBhHbtrV37kuitXlX1xmE/EBThB4KqOvxbK95+Srv21q59SfRWr0p6q/Q7P4DqVL3nB1CRSsJvZivM7LCZvWtmm6roIY+ZHTWzt7OZhyudYiybBu2UmR0Y9dpMM9tjZn/Kfo45TVpFvbXFzM2JmaUr/ezabcbrlh/2m9kUSUck3SdpSNLrkta5+8GWNpLDzI5K6nL3yseEzeyrks5L2jEyG5KZ/UDSGXf/fvaL8wZ3/4826W2zxjlzc5N6y5tZ+huq8LMrc8brMlSx518s6V13/7O7/03SLkmrK+ij7bn7gKQzV728WlJP9rhHw//ztFxOb23B3U+4+5vZ448kjcwsXelnl+irElWE/2ZJx0c9H1J7Tfntkn5vZm+YWXfVzYxhzsjMSNnP2RX3c7XCmZtb6aqZpdvms6tnxuuyVRH+sWYTaachhyXu/s+SVkramB3eojY/lfRlDU/jdkLSj6psJptZulfSt939XJW9jDZGX5V8blWEf0jSLaOef1HS+xX0MSZ3fz/7eUrSbzT8NaWdnByZJDX7earifv6fu59098vufkXSz1ThZ5fNLN0r6Rfu/nz2cuWf3Vh9VfW5VRH+1yXdamYLzGyapK9L6qugj88xs47sDzEysw5JX1P7zT7cJ2l99ni9pBcq7OUz2mXm5ryZpVXxZ9duM15XcpJPNpTxn5KmSNru7t9reRNjMLMvaXhvLw1f8fjLKnszs52S7tHwVV8nJX1H0m8l/VrSPEnHJK1195b/4S2nt3s0zpmbm9Rb3szS+1ThZ1fmjNel9MMZfkBMnOEHBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo/wMJ/DWijmxB/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1830029cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotData = plotData.reshape(28, 28) # reshape to make image a matirx\n",
    "plt.gray() # use this line if you don't want to see it in color\n",
    "plt.imshow(plotData)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a softmax regression. Softmax regression is just a multi-class generalization of the logistic regression model:\n",
    "\n",
    "$Y=softmax(XW+b)$\n",
    "where\n",
    "\n",
    "$Y: N \\times K$. Labels. The labels are one-hot encoded.\n",
    "\n",
    "$X: N \\times M$. Features.\n",
    "\n",
    "$W: M \\times K$. Weights.\n",
    "\n",
    "$b: K$. Bias\n",
    "\n",
    "Here, $N$ can either be the train, validation, test sample size or the mini-batch size, depending on which part of the process you're in. $M$ is the number of features (784 in this case) and $K$ is the number of classes (10 in this case).\n",
    "\n",
    "I really like to write down all the matrix dimensions before I start any coding. This way, I can avoid matrix dimension errors later on. This is kind of like creating an outline of your essay before you start drafting it.\n",
    "\n",
    "The loss we would like to optimize is the cross-entropy loss, averaged over the sample size:\n",
    "\n",
    "$$\\frac{-\\sum_{n=1}^N \\sum_{k=1}^K y_{n,k} log(\\hat{y}_{n,k})}{N}$$\n",
    "\n",
    "We will train this using gradient descent. \n",
    "\n",
    "Coding of this model is a six step process:\n",
    "1. Define variables and placeholders.\n",
    "2. Define the model.\n",
    "3. Define the loss function.\n",
    "4. Define the optimizer.\n",
    "5. Train the model, i.e. initialize variables and run optimizer.\n",
    "6. Evaluate the model.\n",
    "\n",
    "This is the same for any other models. Neural nets will have these steps too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y,X$ will be a placeholder, because we will feed in datasets with different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 784\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, M])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W,b$ will be variables, because we would like to mutate them every epoch via the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([M, K])) # the variables will be all zeros initially\n",
    "b = tf.Variable(tf.zeros([K]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Softmax regression model is a one liner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = tf.nn.softmax(X @W + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "The cross-entropy loss is also a one liner! Make sure to compare it with the equation above and check that it's doing what it's supposed to. We will also define accuracy for evaluating the result on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss               = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(Yhat), axis=1))\n",
    "# choose predictions from 10 classes and compare them with the true labels\n",
    "correct_prediction = tf.equal(tf.argmax(Yhat, 1), tf.argmax(Y, 1))           \n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Here's somethig new to learn! `GradientDescentOptimizer` takes in the objective that we would like to minimize. When we run this optimizer in a session, it will look for all trainable variables (or the specified list of variables, if any) and update them. There are many other optimizers e.g. `AdagradOptimizer`, `AdamOptimizer`. Find your favorite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01 # this is a hyperparameter to tune\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate) # first initialize an optimizer\n",
    "update = optimizer.minimize(loss) # then pass in the objective you would like to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables and run optimizer, evaluate the model\n",
    "The final step is to train the model, and then to evaluate it. We do these in one go. \n",
    "We have 25 epochs. Within each epoch, we go through the dataset batch by batch. In each bath, we `sess.run` two variables: `update` and `loss`. Running `update` applies backpropagation to the graph and updates the variables relevant to minimizing the objective. In this case `W` and `b` are updated. Running `loss` just extracts the objective so that we can keep track of how well our model is being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 1.18426021858\n",
      "Epoch: 1 Cost: 0.665378224416\n",
      "Epoch: 2 Cost: 0.552918814258\n",
      "Epoch: 3 Cost: 0.498641848239\n",
      "Epoch: 4 Cost: 0.465547590039\n",
      "Epoch: 5 Cost: 0.442545095899\n",
      "Epoch: 6 Cost: 0.425528773124\n",
      "Epoch: 7 Cost: 0.412196523954\n",
      "Epoch: 8 Cost: 0.401359560354\n",
      "Epoch: 9 Cost: 0.392407216023\n",
      "Epoch: 10 Cost: 0.384728899219\n",
      "Epoch: 11 Cost: 0.3781267132\n",
      "Epoch: 12 Cost: 0.372398837127\n",
      "Epoch: 13 Cost: 0.367295924154\n",
      "Epoch: 14 Cost: 0.362700622407\n",
      "Epoch: 15 Cost: 0.35860175363\n",
      "Epoch: 16 Cost: 0.354902144264\n",
      "Epoch: 17 Cost: 0.351483033462\n",
      "Epoch: 18 Cost: 0.348351359584\n",
      "Epoch: 19 Cost: 0.345449161638\n",
      "Epoch: 20 Cost: 0.342763654129\n",
      "Epoch: 21 Cost: 0.340286806009\n",
      "Epoch: 22 Cost: 0.337942223278\n",
      "Epoch: 23 Cost: 0.335790792026\n",
      "Epoch: 24 Cost: 0.333662368357\n",
      "Test Accuracy: 0.9139\n"
     ]
    }
   ],
   "source": [
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # remember, you need to initialize variables first\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0 # print cost per epoch\n",
    "        for _ in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([update, loss], feed_dict={X: batch_X, \n",
    "                                                          Y: batch_Y}) # evaluate `update` and `loss`\n",
    "            average_cost += c / total_batch\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "        \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!\n",
    "We have 92% ish accuracy. How good is this? To cite the `TensorFlow` documentation:\n",
    "\n",
    "> Getting 92% accuracy on MNIST is bad. It's almost embarrassingly bad. In this section, we'll fix that, jumping from a very simple model to something moderately sophisticated: a small convolutional neural network. This will get us to around 99.2% accuracy -- not state of the art, but respectable.\n",
    "\n",
    "We are not going to introduce CNNs in this workshop, but you should take a look to aim for 99+% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Train a linear regression using gradient descent (no closed form!) that predicts housing prices in Boston from other covariates provided. Follow the six steps above. Plot the loss and observe that it goes down.\n",
    "\n",
    "(Optional) Fit a LASSO. Fit a Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data)\n",
    "df.columns = boston.feature_names\n",
    "df['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = df.shape[1]\n",
    "N = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, M])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.ones([M, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = X @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean((Y-Yhat)**2)\n",
    "# regularizer = tf.contrib.layers.l1_regularizer(scale=0.1)\n",
    "# regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)\n",
    "# regularization_penalty = tf.contrib.layers.apply_regularization(regularizer, [W])\n",
    "loss += regularization_penalty\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = df.iloc[:,:-1].as_matrix()\n",
    "X_np = (X_np - np.mean(X_np,axis=0)) / np.std(X_np,axis=0)\n",
    "X_np = np.hstack((X_np,np.ones([X_np.shape[0],1])))\n",
    "y_np = df.iloc[:,-1].as_matrix()[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.rand(X_np.shape[0]) < 0.80\n",
    "X_train = X_np[train_indices]\n",
    "X_test  = X_np[~train_indices]\n",
    "y_train = y_np[train_indices]\n",
    "y_test  = y_np[~train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 598.612\n",
      "Epoch: 1 Cost: 558.381\n",
      "Epoch: 2 Cost: 523.979\n",
      "Epoch: 3 Cost: 494.165\n",
      "Epoch: 4 Cost: 467.984\n",
      "Epoch: 5 Cost: 444.709\n",
      "Epoch: 6 Cost: 423.776\n",
      "Epoch: 7 Cost: 404.753\n",
      "Epoch: 8 Cost: 387.317\n",
      "Epoch: 9 Cost: 371.218\n",
      "Epoch: 10 Cost: 356.228\n",
      "Epoch: 11 Cost: 342.2\n",
      "Epoch: 12 Cost: 329.008\n",
      "Epoch: 13 Cost: 316.56\n",
      "Epoch: 14 Cost: 304.77\n",
      "Epoch: 15 Cost: 293.566\n",
      "Epoch: 16 Cost: 282.89\n",
      "Epoch: 17 Cost: 272.701\n",
      "Epoch: 18 Cost: 262.965\n",
      "Epoch: 19 Cost: 253.649\n",
      "Epoch: 20 Cost: 244.732\n",
      "Epoch: 21 Cost: 236.192\n",
      "Epoch: 22 Cost: 228.004\n",
      "Epoch: 23 Cost: 220.149\n",
      "Epoch: 24 Cost: 212.613\n",
      "Epoch: 25 Cost: 205.38\n",
      "Epoch: 26 Cost: 198.437\n",
      "Epoch: 27 Cost: 191.77\n",
      "Epoch: 28 Cost: 185.369\n",
      "Epoch: 29 Cost: 179.221\n",
      "Epoch: 30 Cost: 173.316\n",
      "Epoch: 31 Cost: 167.644\n",
      "Epoch: 32 Cost: 162.196\n",
      "Epoch: 33 Cost: 156.962\n",
      "Epoch: 34 Cost: 151.935\n",
      "Epoch: 35 Cost: 147.104\n",
      "Epoch: 36 Cost: 142.463\n",
      "Epoch: 37 Cost: 138.004\n",
      "Epoch: 38 Cost: 133.72\n",
      "Epoch: 39 Cost: 129.604\n",
      "Epoch: 40 Cost: 125.649\n",
      "Epoch: 41 Cost: 121.848\n",
      "Epoch: 42 Cost: 118.196\n",
      "Epoch: 43 Cost: 114.687\n",
      "Epoch: 44 Cost: 111.315\n",
      "Epoch: 45 Cost: 108.074\n",
      "Epoch: 46 Cost: 104.96\n",
      "Epoch: 47 Cost: 101.967\n",
      "Epoch: 48 Cost: 99.0912\n",
      "Epoch: 49 Cost: 96.3272\n",
      "Epoch: 50 Cost: 93.6707\n",
      "Epoch: 51 Cost: 91.1176\n",
      "Epoch: 52 Cost: 88.6638\n",
      "Epoch: 53 Cost: 86.3053\n",
      "Epoch: 54 Cost: 84.0385\n",
      "Epoch: 55 Cost: 81.8596\n",
      "Epoch: 56 Cost: 79.7653\n",
      "Epoch: 57 Cost: 77.7522\n",
      "Epoch: 58 Cost: 75.8171\n",
      "Epoch: 59 Cost: 73.957\n",
      "Epoch: 60 Cost: 72.1688\n",
      "Epoch: 61 Cost: 70.4498\n",
      "Epoch: 62 Cost: 68.7971\n",
      "Epoch: 63 Cost: 67.2084\n",
      "Epoch: 64 Cost: 65.6809\n",
      "Epoch: 65 Cost: 64.2123\n",
      "Epoch: 66 Cost: 62.8003\n",
      "Epoch: 67 Cost: 61.4445\n",
      "Epoch: 68 Cost: 60.1468\n",
      "Epoch: 69 Cost: 58.8992\n",
      "Epoch: 70 Cost: 57.6995\n",
      "Epoch: 71 Cost: 56.546\n",
      "Epoch: 72 Cost: 55.4368\n",
      "Epoch: 73 Cost: 54.3702\n",
      "Epoch: 74 Cost: 53.3446\n",
      "Epoch: 75 Cost: 52.3596\n",
      "Epoch: 76 Cost: 51.4125\n",
      "Epoch: 77 Cost: 50.5017\n",
      "Epoch: 78 Cost: 49.6258\n",
      "Epoch: 79 Cost: 48.7834\n",
      "Epoch: 80 Cost: 47.9732\n",
      "Epoch: 81 Cost: 47.1939\n",
      "Epoch: 82 Cost: 46.4445\n",
      "Epoch: 83 Cost: 45.7235\n",
      "Epoch: 84 Cost: 45.03\n",
      "Epoch: 85 Cost: 44.363\n",
      "Epoch: 86 Cost: 43.7212\n",
      "Epoch: 87 Cost: 43.1038\n",
      "Epoch: 88 Cost: 42.5098\n",
      "Epoch: 89 Cost: 41.9383\n",
      "Epoch: 90 Cost: 41.3884\n",
      "Epoch: 91 Cost: 40.8592\n",
      "Epoch: 92 Cost: 40.35\n",
      "Epoch: 93 Cost: 39.86\n",
      "Epoch: 94 Cost: 39.3884\n",
      "Epoch: 95 Cost: 38.9345\n",
      "Epoch: 96 Cost: 38.4977\n",
      "Epoch: 97 Cost: 38.0772\n",
      "Epoch: 98 Cost: 37.6724\n",
      "Epoch: 99 Cost: 37.2827\n",
      "Epoch: 100 Cost: 36.9075\n",
      "Epoch: 101 Cost: 36.5463\n",
      "Epoch: 102 Cost: 36.1985\n",
      "Epoch: 103 Cost: 35.8636\n",
      "Epoch: 104 Cost: 35.5411\n",
      "Epoch: 105 Cost: 35.2305\n",
      "Epoch: 106 Cost: 34.9314\n",
      "Epoch: 107 Cost: 34.6432\n",
      "Epoch: 108 Cost: 34.3657\n",
      "Epoch: 109 Cost: 34.0983\n",
      "Epoch: 110 Cost: 33.8406\n",
      "Epoch: 111 Cost: 33.5924\n",
      "Epoch: 112 Cost: 33.3532\n",
      "Epoch: 113 Cost: 33.1227\n",
      "Epoch: 114 Cost: 32.9006\n",
      "Epoch: 115 Cost: 32.6865\n",
      "Epoch: 116 Cost: 32.4801\n",
      "Epoch: 117 Cost: 32.2811\n",
      "Epoch: 118 Cost: 32.0892\n",
      "Epoch: 119 Cost: 31.9043\n",
      "Epoch: 120 Cost: 31.7259\n",
      "Epoch: 121 Cost: 31.554\n",
      "Epoch: 122 Cost: 31.3889\n",
      "Epoch: 123 Cost: 31.2297\n",
      "Epoch: 124 Cost: 31.0762\n",
      "Epoch: 125 Cost: 30.9282\n",
      "Epoch: 126 Cost: 30.7853\n",
      "Epoch: 127 Cost: 30.6475\n",
      "Epoch: 128 Cost: 30.5145\n",
      "Epoch: 129 Cost: 30.3862\n",
      "Epoch: 130 Cost: 30.2624\n",
      "Epoch: 131 Cost: 30.1428\n",
      "Epoch: 132 Cost: 30.0274\n",
      "Epoch: 133 Cost: 29.916\n",
      "Epoch: 134 Cost: 29.8084\n",
      "Epoch: 135 Cost: 29.7045\n",
      "Epoch: 136 Cost: 29.6041\n",
      "Epoch: 137 Cost: 29.5071\n",
      "Epoch: 138 Cost: 29.4134\n",
      "Epoch: 139 Cost: 29.3229\n",
      "Epoch: 140 Cost: 29.2354\n",
      "Epoch: 141 Cost: 29.1508\n",
      "Epoch: 142 Cost: 29.069\n",
      "Epoch: 143 Cost: 28.9899\n",
      "Epoch: 144 Cost: 28.9135\n",
      "Epoch: 145 Cost: 28.8395\n",
      "Epoch: 146 Cost: 28.7679\n",
      "Epoch: 147 Cost: 28.6987\n",
      "Epoch: 148 Cost: 28.6317\n",
      "Epoch: 149 Cost: 28.5668\n",
      "Epoch: 150 Cost: 28.504\n",
      "Epoch: 151 Cost: 28.4433\n",
      "Epoch: 152 Cost: 28.3844\n",
      "Epoch: 153 Cost: 28.3273\n",
      "Epoch: 154 Cost: 28.2721\n",
      "Epoch: 155 Cost: 28.2186\n",
      "Epoch: 156 Cost: 28.1667\n",
      "Epoch: 157 Cost: 28.1164\n",
      "Epoch: 158 Cost: 28.0676\n",
      "Epoch: 159 Cost: 28.0203\n",
      "Epoch: 160 Cost: 27.9744\n",
      "Epoch: 161 Cost: 27.9299\n",
      "Epoch: 162 Cost: 27.8868\n",
      "Epoch: 163 Cost: 27.8448\n",
      "Epoch: 164 Cost: 27.8041\n",
      "Epoch: 165 Cost: 27.7646\n",
      "Epoch: 166 Cost: 27.7263\n",
      "Epoch: 167 Cost: 27.689\n",
      "Epoch: 168 Cost: 27.6528\n",
      "Epoch: 169 Cost: 27.6176\n",
      "Epoch: 170 Cost: 27.5833\n",
      "Epoch: 171 Cost: 27.5501\n",
      "Epoch: 172 Cost: 27.5177\n",
      "Epoch: 173 Cost: 27.4862\n",
      "Epoch: 174 Cost: 27.4556\n",
      "Epoch: 175 Cost: 27.4258\n",
      "Epoch: 176 Cost: 27.3967\n",
      "Epoch: 177 Cost: 27.3685\n",
      "Epoch: 178 Cost: 27.341\n",
      "Epoch: 179 Cost: 27.3142\n",
      "Epoch: 180 Cost: 27.288\n",
      "Epoch: 181 Cost: 27.2626\n",
      "Epoch: 182 Cost: 27.2377\n",
      "Epoch: 183 Cost: 27.2135\n",
      "Epoch: 184 Cost: 27.1899\n",
      "Epoch: 185 Cost: 27.1669\n",
      "Epoch: 186 Cost: 27.1444\n",
      "Epoch: 187 Cost: 27.1225\n",
      "Epoch: 188 Cost: 27.1011\n",
      "Epoch: 189 Cost: 27.0801\n",
      "Epoch: 190 Cost: 27.0597\n",
      "Epoch: 191 Cost: 27.0397\n",
      "Epoch: 192 Cost: 27.0202\n",
      "Epoch: 193 Cost: 27.0012\n",
      "Epoch: 194 Cost: 26.9825\n",
      "Epoch: 195 Cost: 26.9643\n",
      "Epoch: 196 Cost: 26.9464\n",
      "Epoch: 197 Cost: 26.929\n",
      "Epoch: 198 Cost: 26.9119\n",
      "Epoch: 199 Cost: 26.8952\n",
      "Epoch: 200 Cost: 26.8788\n",
      "Epoch: 201 Cost: 26.8627\n",
      "Epoch: 202 Cost: 26.847\n",
      "Epoch: 203 Cost: 26.8316\n",
      "Epoch: 204 Cost: 26.8165\n",
      "Epoch: 205 Cost: 26.8017\n",
      "Epoch: 206 Cost: 26.7872\n",
      "Epoch: 207 Cost: 26.773\n",
      "Epoch: 208 Cost: 26.759\n",
      "Epoch: 209 Cost: 26.7453\n",
      "Epoch: 210 Cost: 26.7319\n",
      "Epoch: 211 Cost: 26.7187\n",
      "Epoch: 212 Cost: 26.7057\n",
      "Epoch: 213 Cost: 26.693\n",
      "Epoch: 214 Cost: 26.6805\n",
      "Epoch: 215 Cost: 26.6682\n",
      "Epoch: 216 Cost: 26.6561\n",
      "Epoch: 217 Cost: 26.6442\n",
      "Epoch: 218 Cost: 26.6326\n",
      "Epoch: 219 Cost: 26.6211\n",
      "Epoch: 220 Cost: 26.6098\n",
      "Epoch: 221 Cost: 26.5987\n",
      "Epoch: 222 Cost: 26.5878\n",
      "Epoch: 223 Cost: 26.5771\n",
      "Epoch: 224 Cost: 26.5665\n",
      "Epoch: 225 Cost: 26.5561\n",
      "Epoch: 226 Cost: 26.5458\n",
      "Epoch: 227 Cost: 26.5357\n",
      "Epoch: 228 Cost: 26.5258\n",
      "Epoch: 229 Cost: 26.516\n",
      "Epoch: 230 Cost: 26.5063\n",
      "Epoch: 231 Cost: 26.4968\n",
      "Epoch: 232 Cost: 26.4874\n",
      "Epoch: 233 Cost: 26.4782\n",
      "Epoch: 234 Cost: 26.4691\n",
      "Epoch: 235 Cost: 26.4601\n",
      "Epoch: 236 Cost: 26.4512\n",
      "Epoch: 237 Cost: 26.4425\n",
      "Epoch: 238 Cost: 26.4339\n",
      "Epoch: 239 Cost: 26.4253\n",
      "Epoch: 240 Cost: 26.4169\n",
      "Epoch: 241 Cost: 26.4086\n",
      "Epoch: 242 Cost: 26.4005\n",
      "Epoch: 243 Cost: 26.3924\n",
      "Epoch: 244 Cost: 26.3844\n",
      "Epoch: 245 Cost: 26.3765\n",
      "Epoch: 246 Cost: 26.3687\n",
      "Epoch: 247 Cost: 26.3611\n",
      "Epoch: 248 Cost: 26.3535\n",
      "Epoch: 249 Cost: 26.346\n",
      "Epoch: 250 Cost: 26.3386\n",
      "Epoch: 251 Cost: 26.3312\n",
      "Epoch: 252 Cost: 26.324\n",
      "Epoch: 253 Cost: 26.3168\n",
      "Epoch: 254 Cost: 26.3098\n",
      "Epoch: 255 Cost: 26.3028\n",
      "Epoch: 256 Cost: 26.2958\n",
      "Epoch: 257 Cost: 26.289\n",
      "Epoch: 258 Cost: 26.2822\n",
      "Epoch: 259 Cost: 26.2755\n",
      "Epoch: 260 Cost: 26.2689\n",
      "Epoch: 261 Cost: 26.2624\n",
      "Epoch: 262 Cost: 26.2559\n",
      "Epoch: 263 Cost: 26.2495\n",
      "Epoch: 264 Cost: 26.2432\n",
      "Epoch: 265 Cost: 26.2369\n",
      "Epoch: 266 Cost: 26.2307\n",
      "Epoch: 267 Cost: 26.2245\n",
      "Epoch: 268 Cost: 26.2185\n",
      "Epoch: 269 Cost: 26.2124\n",
      "Epoch: 270 Cost: 26.2065\n",
      "Epoch: 271 Cost: 26.2006\n",
      "Epoch: 272 Cost: 26.1947\n",
      "Epoch: 273 Cost: 26.1889\n",
      "Epoch: 274 Cost: 26.1832\n",
      "Epoch: 275 Cost: 26.1775\n",
      "Epoch: 276 Cost: 26.1719\n",
      "Epoch: 277 Cost: 26.1663\n",
      "Epoch: 278 Cost: 26.1608\n",
      "Epoch: 279 Cost: 26.1554\n",
      "Epoch: 280 Cost: 26.15\n",
      "Epoch: 281 Cost: 26.1446\n",
      "Epoch: 282 Cost: 26.1393\n",
      "Epoch: 283 Cost: 26.134\n",
      "Epoch: 284 Cost: 26.1288\n",
      "Epoch: 285 Cost: 26.1236\n",
      "Epoch: 286 Cost: 26.1185\n",
      "Epoch: 287 Cost: 26.1135\n",
      "Epoch: 288 Cost: 26.1084\n",
      "Epoch: 289 Cost: 26.1034\n",
      "Epoch: 290 Cost: 26.0985\n",
      "Epoch: 291 Cost: 26.0936\n",
      "Epoch: 292 Cost: 26.0888\n",
      "Epoch: 293 Cost: 26.0839\n",
      "Epoch: 294 Cost: 26.0792\n",
      "Epoch: 295 Cost: 26.0744\n",
      "Epoch: 296 Cost: 26.0698\n",
      "Epoch: 297 Cost: 26.0651\n",
      "Epoch: 298 Cost: 26.0605\n",
      "Epoch: 299 Cost: 26.0559\n",
      "Epoch: 300 Cost: 26.0514\n",
      "Epoch: 301 Cost: 26.0469\n",
      "Epoch: 302 Cost: 26.0424\n",
      "Epoch: 303 Cost: 26.038\n",
      "Epoch: 304 Cost: 26.0337\n",
      "Epoch: 305 Cost: 26.0293\n",
      "Epoch: 306 Cost: 26.025\n",
      "Epoch: 307 Cost: 26.0207\n",
      "Epoch: 308 Cost: 26.0165\n",
      "Epoch: 309 Cost: 26.0123\n",
      "Epoch: 310 Cost: 26.0081\n",
      "Epoch: 311 Cost: 26.004\n",
      "Epoch: 312 Cost: 25.9999\n",
      "Epoch: 313 Cost: 25.9958\n",
      "Epoch: 314 Cost: 25.9917\n",
      "Epoch: 315 Cost: 25.9877\n",
      "Epoch: 316 Cost: 25.9838\n",
      "Epoch: 317 Cost: 25.9798\n",
      "Epoch: 318 Cost: 25.9759\n",
      "Epoch: 319 Cost: 25.972\n",
      "Epoch: 320 Cost: 25.9682\n",
      "Epoch: 321 Cost: 25.9643\n",
      "Epoch: 322 Cost: 25.9606\n",
      "Epoch: 323 Cost: 25.9568\n",
      "Epoch: 324 Cost: 25.9531\n",
      "Epoch: 325 Cost: 25.9493\n",
      "Epoch: 326 Cost: 25.9457\n",
      "Epoch: 327 Cost: 25.942\n",
      "Epoch: 328 Cost: 25.9384\n",
      "Epoch: 329 Cost: 25.9348\n",
      "Epoch: 330 Cost: 25.9312\n",
      "Epoch: 331 Cost: 25.9277\n",
      "Epoch: 332 Cost: 25.9242\n",
      "Epoch: 333 Cost: 25.9207\n",
      "Epoch: 334 Cost: 25.9172\n",
      "Epoch: 335 Cost: 25.9138\n",
      "Epoch: 336 Cost: 25.9104\n",
      "Epoch: 337 Cost: 25.907\n",
      "Epoch: 338 Cost: 25.9036\n",
      "Epoch: 339 Cost: 25.9003\n",
      "Epoch: 340 Cost: 25.897\n",
      "Epoch: 341 Cost: 25.8937\n",
      "Epoch: 342 Cost: 25.8905\n",
      "Epoch: 343 Cost: 25.8872\n",
      "Epoch: 344 Cost: 25.884\n",
      "Epoch: 345 Cost: 25.8808\n",
      "Epoch: 346 Cost: 25.8777\n",
      "Epoch: 347 Cost: 25.8745\n",
      "Epoch: 348 Cost: 25.8714\n",
      "Epoch: 349 Cost: 25.8683\n",
      "Epoch: 350 Cost: 25.8652\n",
      "Epoch: 351 Cost: 25.8622\n",
      "Epoch: 352 Cost: 25.8591\n",
      "Epoch: 353 Cost: 25.8561\n",
      "Epoch: 354 Cost: 25.8531\n",
      "Epoch: 355 Cost: 25.8502\n",
      "Epoch: 356 Cost: 25.8472\n",
      "Epoch: 357 Cost: 25.8443\n",
      "Epoch: 358 Cost: 25.8414\n",
      "Epoch: 359 Cost: 25.8385\n",
      "Epoch: 360 Cost: 25.8356\n",
      "Epoch: 361 Cost: 25.8328\n",
      "Epoch: 362 Cost: 25.83\n",
      "Epoch: 363 Cost: 25.8272\n",
      "Epoch: 364 Cost: 25.8244\n",
      "Epoch: 365 Cost: 25.8216\n",
      "Epoch: 366 Cost: 25.8189\n",
      "Epoch: 367 Cost: 25.8162\n",
      "Epoch: 368 Cost: 25.8135\n",
      "Epoch: 369 Cost: 25.8108\n",
      "Epoch: 370 Cost: 25.8081\n",
      "Epoch: 371 Cost: 25.8054\n",
      "Epoch: 372 Cost: 25.8028\n",
      "Epoch: 373 Cost: 25.8002\n",
      "Epoch: 374 Cost: 25.7976\n",
      "Epoch: 375 Cost: 25.795\n",
      "Epoch: 376 Cost: 25.7925\n",
      "Epoch: 377 Cost: 25.7899\n",
      "Epoch: 378 Cost: 25.7874\n",
      "Epoch: 379 Cost: 25.7849\n",
      "Epoch: 380 Cost: 25.7824\n",
      "Epoch: 381 Cost: 25.7799\n",
      "Epoch: 382 Cost: 25.7775\n",
      "Epoch: 383 Cost: 25.775\n",
      "Epoch: 384 Cost: 25.7726\n",
      "Epoch: 385 Cost: 25.7702\n",
      "Epoch: 386 Cost: 25.7678\n",
      "Epoch: 387 Cost: 25.7655\n",
      "Epoch: 388 Cost: 25.7631\n",
      "Epoch: 389 Cost: 25.7607\n",
      "Epoch: 390 Cost: 25.7584\n",
      "Epoch: 391 Cost: 25.7561\n",
      "Epoch: 392 Cost: 25.7538\n",
      "Epoch: 393 Cost: 25.7515\n",
      "Epoch: 394 Cost: 25.7493\n",
      "Epoch: 395 Cost: 25.747\n",
      "Epoch: 396 Cost: 25.7448\n",
      "Epoch: 397 Cost: 25.7426\n",
      "Epoch: 398 Cost: 25.7404\n",
      "Epoch: 399 Cost: 25.7382\n",
      "Epoch: 400 Cost: 25.736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 401 Cost: 25.7338\n",
      "Epoch: 402 Cost: 25.7317\n",
      "Epoch: 403 Cost: 25.7296\n",
      "Epoch: 404 Cost: 25.7274\n",
      "Epoch: 405 Cost: 25.7253\n",
      "Epoch: 406 Cost: 25.7233\n",
      "Epoch: 407 Cost: 25.7212\n",
      "Epoch: 408 Cost: 25.7191\n",
      "Epoch: 409 Cost: 25.7171\n",
      "Epoch: 410 Cost: 25.715\n",
      "Epoch: 411 Cost: 25.713\n",
      "Epoch: 412 Cost: 25.711\n",
      "Epoch: 413 Cost: 25.709\n",
      "Epoch: 414 Cost: 25.707\n",
      "Epoch: 415 Cost: 25.7051\n",
      "Epoch: 416 Cost: 25.7031\n",
      "Epoch: 417 Cost: 25.7012\n",
      "Epoch: 418 Cost: 25.6992\n",
      "Epoch: 419 Cost: 25.6973\n",
      "Epoch: 420 Cost: 25.6954\n",
      "Epoch: 421 Cost: 25.6935\n",
      "Epoch: 422 Cost: 25.6916\n",
      "Epoch: 423 Cost: 25.6897\n",
      "Epoch: 424 Cost: 25.6879\n",
      "Epoch: 425 Cost: 25.686\n",
      "Epoch: 426 Cost: 25.6842\n",
      "Epoch: 427 Cost: 25.6824\n",
      "Epoch: 428 Cost: 25.6806\n",
      "Epoch: 429 Cost: 25.6788\n",
      "Epoch: 430 Cost: 25.677\n",
      "Epoch: 431 Cost: 25.6752\n",
      "Epoch: 432 Cost: 25.6734\n",
      "Epoch: 433 Cost: 25.6717\n",
      "Epoch: 434 Cost: 25.67\n",
      "Epoch: 435 Cost: 25.6682\n",
      "Epoch: 436 Cost: 25.6665\n",
      "Epoch: 437 Cost: 25.6648\n",
      "Epoch: 438 Cost: 25.6631\n",
      "Epoch: 439 Cost: 25.6614\n",
      "Epoch: 440 Cost: 25.6597\n",
      "Epoch: 441 Cost: 25.6581\n",
      "Epoch: 442 Cost: 25.6564\n",
      "Epoch: 443 Cost: 25.6548\n",
      "Epoch: 444 Cost: 25.6531\n",
      "Epoch: 445 Cost: 25.6515\n",
      "Epoch: 446 Cost: 25.6499\n",
      "Epoch: 447 Cost: 25.6483\n",
      "Epoch: 448 Cost: 25.6467\n",
      "Epoch: 449 Cost: 25.6451\n",
      "Epoch: 450 Cost: 25.6435\n",
      "Epoch: 451 Cost: 25.642\n",
      "Epoch: 452 Cost: 25.6404\n",
      "Epoch: 453 Cost: 25.6389\n",
      "Epoch: 454 Cost: 25.6373\n",
      "Epoch: 455 Cost: 25.6358\n",
      "Epoch: 456 Cost: 25.6343\n",
      "Epoch: 457 Cost: 25.6328\n",
      "Epoch: 458 Cost: 25.6313\n",
      "Epoch: 459 Cost: 25.6298\n",
      "Epoch: 460 Cost: 25.6283\n",
      "Epoch: 461 Cost: 25.6268\n",
      "Epoch: 462 Cost: 25.6254\n",
      "Epoch: 463 Cost: 25.6239\n",
      "Epoch: 464 Cost: 25.6225\n",
      "Epoch: 465 Cost: 25.6211\n",
      "Epoch: 466 Cost: 25.6196\n",
      "Epoch: 467 Cost: 25.6182\n",
      "Epoch: 468 Cost: 25.6168\n",
      "Epoch: 469 Cost: 25.6154\n",
      "Epoch: 470 Cost: 25.614\n",
      "Epoch: 471 Cost: 25.6126\n",
      "Epoch: 472 Cost: 25.6113\n",
      "Epoch: 473 Cost: 25.6099\n",
      "Epoch: 474 Cost: 25.6085\n",
      "Epoch: 475 Cost: 25.6072\n",
      "Epoch: 476 Cost: 25.6058\n",
      "Epoch: 477 Cost: 25.6045\n",
      "Epoch: 478 Cost: 25.6032\n",
      "Epoch: 479 Cost: 25.6019\n",
      "Epoch: 480 Cost: 25.6006\n",
      "Epoch: 481 Cost: 25.5993\n",
      "Epoch: 482 Cost: 25.598\n",
      "Epoch: 483 Cost: 25.5967\n",
      "Epoch: 484 Cost: 25.5954\n",
      "Epoch: 485 Cost: 25.5941\n",
      "Epoch: 486 Cost: 25.5929\n",
      "Epoch: 487 Cost: 25.5916\n",
      "Epoch: 488 Cost: 25.5904\n",
      "Epoch: 489 Cost: 25.5891\n",
      "Epoch: 490 Cost: 25.5879\n",
      "Epoch: 491 Cost: 25.5867\n",
      "Epoch: 492 Cost: 25.5855\n",
      "Epoch: 493 Cost: 25.5842\n",
      "Epoch: 494 Cost: 25.583\n",
      "Epoch: 495 Cost: 25.5818\n",
      "Epoch: 496 Cost: 25.5807\n",
      "Epoch: 497 Cost: 25.5795\n",
      "Epoch: 498 Cost: 25.5783\n",
      "Epoch: 499 Cost: 25.5771\n",
      "Epoch: 500 Cost: 25.576\n",
      "Epoch: 501 Cost: 25.5748\n",
      "Epoch: 502 Cost: 25.5737\n",
      "Epoch: 503 Cost: 25.5725\n",
      "Epoch: 504 Cost: 25.5714\n",
      "Epoch: 505 Cost: 25.5703\n",
      "Epoch: 506 Cost: 25.5692\n",
      "Epoch: 507 Cost: 25.568\n",
      "Epoch: 508 Cost: 25.5669\n",
      "Epoch: 509 Cost: 25.5658\n",
      "Epoch: 510 Cost: 25.5647\n",
      "Epoch: 511 Cost: 25.5637\n",
      "Epoch: 512 Cost: 25.5626\n",
      "Epoch: 513 Cost: 25.5615\n",
      "Epoch: 514 Cost: 25.5604\n",
      "Epoch: 515 Cost: 25.5594\n",
      "Epoch: 516 Cost: 25.5583\n",
      "Epoch: 517 Cost: 25.5573\n",
      "Epoch: 518 Cost: 25.5562\n",
      "Epoch: 519 Cost: 25.5552\n",
      "Epoch: 520 Cost: 25.5541\n",
      "Epoch: 521 Cost: 25.5531\n",
      "Epoch: 522 Cost: 25.5521\n",
      "Epoch: 523 Cost: 25.5511\n",
      "Epoch: 524 Cost: 25.5501\n",
      "Epoch: 525 Cost: 25.5491\n",
      "Epoch: 526 Cost: 25.5481\n",
      "Epoch: 527 Cost: 25.5471\n",
      "Epoch: 528 Cost: 25.5461\n",
      "Epoch: 529 Cost: 25.5451\n",
      "Epoch: 530 Cost: 25.5442\n",
      "Epoch: 531 Cost: 25.5432\n",
      "Epoch: 532 Cost: 25.5422\n",
      "Epoch: 533 Cost: 25.5413\n",
      "Epoch: 534 Cost: 25.5403\n",
      "Epoch: 535 Cost: 25.5394\n",
      "Epoch: 536 Cost: 25.5384\n",
      "Epoch: 537 Cost: 25.5375\n",
      "Epoch: 538 Cost: 25.5366\n",
      "Epoch: 539 Cost: 25.5356\n",
      "Epoch: 540 Cost: 25.5347\n",
      "Epoch: 541 Cost: 25.5338\n",
      "Epoch: 542 Cost: 25.5329\n",
      "Epoch: 543 Cost: 25.532\n",
      "Epoch: 544 Cost: 25.5311\n",
      "Epoch: 545 Cost: 25.5302\n",
      "Epoch: 546 Cost: 25.5293\n",
      "Epoch: 547 Cost: 25.5284\n",
      "Epoch: 548 Cost: 25.5275\n",
      "Epoch: 549 Cost: 25.5267\n",
      "Epoch: 550 Cost: 25.5258\n",
      "Epoch: 551 Cost: 25.5249\n",
      "Epoch: 552 Cost: 25.5241\n",
      "Epoch: 553 Cost: 25.5232\n",
      "Epoch: 554 Cost: 25.5224\n",
      "Epoch: 555 Cost: 25.5215\n",
      "Epoch: 556 Cost: 25.5207\n",
      "Epoch: 557 Cost: 25.5199\n",
      "Epoch: 558 Cost: 25.519\n",
      "Epoch: 559 Cost: 25.5182\n",
      "Epoch: 560 Cost: 25.5174\n",
      "Epoch: 561 Cost: 25.5166\n",
      "Epoch: 562 Cost: 25.5158\n",
      "Epoch: 563 Cost: 25.5149\n",
      "Epoch: 564 Cost: 25.5141\n",
      "Epoch: 565 Cost: 25.5133\n",
      "Epoch: 566 Cost: 25.5125\n",
      "Epoch: 567 Cost: 25.5118\n",
      "Epoch: 568 Cost: 25.511\n",
      "Epoch: 569 Cost: 25.5102\n",
      "Epoch: 570 Cost: 25.5094\n",
      "Epoch: 571 Cost: 25.5086\n",
      "Epoch: 572 Cost: 25.5079\n",
      "Epoch: 573 Cost: 25.5071\n",
      "Epoch: 574 Cost: 25.5064\n",
      "Epoch: 575 Cost: 25.5056\n",
      "Epoch: 576 Cost: 25.5048\n",
      "Epoch: 577 Cost: 25.5041\n",
      "Epoch: 578 Cost: 25.5034\n",
      "Epoch: 579 Cost: 25.5026\n",
      "Epoch: 580 Cost: 25.5019\n",
      "Epoch: 581 Cost: 25.5011\n",
      "Epoch: 582 Cost: 25.5004\n",
      "Epoch: 583 Cost: 25.4997\n",
      "Epoch: 584 Cost: 25.499\n",
      "Epoch: 585 Cost: 25.4983\n",
      "Epoch: 586 Cost: 25.4976\n",
      "Epoch: 587 Cost: 25.4968\n",
      "Epoch: 588 Cost: 25.4961\n",
      "Epoch: 589 Cost: 25.4954\n",
      "Epoch: 590 Cost: 25.4947\n",
      "Epoch: 591 Cost: 25.4941\n",
      "Epoch: 592 Cost: 25.4934\n",
      "Epoch: 593 Cost: 25.4927\n",
      "Epoch: 594 Cost: 25.492\n",
      "Epoch: 595 Cost: 25.4913\n",
      "Epoch: 596 Cost: 25.4906\n",
      "Epoch: 597 Cost: 25.49\n",
      "Epoch: 598 Cost: 25.4893\n",
      "Epoch: 599 Cost: 25.4886\n",
      "Epoch: 600 Cost: 25.488\n",
      "Epoch: 601 Cost: 25.4873\n",
      "Epoch: 602 Cost: 25.4867\n",
      "Epoch: 603 Cost: 25.486\n",
      "Epoch: 604 Cost: 25.4854\n",
      "Epoch: 605 Cost: 25.4847\n",
      "Epoch: 606 Cost: 25.4841\n",
      "Epoch: 607 Cost: 25.4835\n",
      "Epoch: 608 Cost: 25.4828\n",
      "Epoch: 609 Cost: 25.4822\n",
      "Epoch: 610 Cost: 25.4816\n",
      "Epoch: 611 Cost: 25.481\n",
      "Epoch: 612 Cost: 25.4803\n",
      "Epoch: 613 Cost: 25.4797\n",
      "Epoch: 614 Cost: 25.4791\n",
      "Epoch: 615 Cost: 25.4785\n",
      "Epoch: 616 Cost: 25.4779\n",
      "Epoch: 617 Cost: 25.4773\n",
      "Epoch: 618 Cost: 25.4767\n",
      "Epoch: 619 Cost: 25.4761\n",
      "Epoch: 620 Cost: 25.4755\n",
      "Epoch: 621 Cost: 25.4749\n",
      "Epoch: 622 Cost: 25.4743\n",
      "Epoch: 623 Cost: 25.4737\n",
      "Epoch: 624 Cost: 25.4732\n",
      "Epoch: 625 Cost: 25.4726\n",
      "Epoch: 626 Cost: 25.472\n",
      "Epoch: 627 Cost: 25.4714\n",
      "Epoch: 628 Cost: 25.4709\n",
      "Epoch: 629 Cost: 25.4703\n",
      "Epoch: 630 Cost: 25.4697\n",
      "Epoch: 631 Cost: 25.4692\n",
      "Epoch: 632 Cost: 25.4686\n",
      "Epoch: 633 Cost: 25.4681\n",
      "Epoch: 634 Cost: 25.4675\n",
      "Epoch: 635 Cost: 25.467\n",
      "Epoch: 636 Cost: 25.4664\n",
      "Epoch: 637 Cost: 25.4659\n",
      "Epoch: 638 Cost: 25.4653\n",
      "Epoch: 639 Cost: 25.4648\n",
      "Epoch: 640 Cost: 25.4643\n",
      "Epoch: 641 Cost: 25.4637\n",
      "Epoch: 642 Cost: 25.4632\n",
      "Epoch: 643 Cost: 25.4627\n",
      "Epoch: 644 Cost: 25.4622\n",
      "Epoch: 645 Cost: 25.4616\n",
      "Epoch: 646 Cost: 25.4611\n",
      "Epoch: 647 Cost: 25.4606\n",
      "Epoch: 648 Cost: 25.4601\n",
      "Epoch: 649 Cost: 25.4596\n",
      "Epoch: 650 Cost: 25.4591\n",
      "Epoch: 651 Cost: 25.4586\n",
      "Epoch: 652 Cost: 25.4581\n",
      "Epoch: 653 Cost: 25.4576\n",
      "Epoch: 654 Cost: 25.4571\n",
      "Epoch: 655 Cost: 25.4566\n",
      "Epoch: 656 Cost: 25.4561\n",
      "Epoch: 657 Cost: 25.4556\n",
      "Epoch: 658 Cost: 25.4551\n",
      "Epoch: 659 Cost: 25.4546\n",
      "Epoch: 660 Cost: 25.4541\n",
      "Epoch: 661 Cost: 25.4537\n",
      "Epoch: 662 Cost: 25.4532\n",
      "Epoch: 663 Cost: 25.4527\n",
      "Epoch: 664 Cost: 25.4522\n",
      "Epoch: 665 Cost: 25.4518\n",
      "Epoch: 666 Cost: 25.4513\n",
      "Epoch: 667 Cost: 25.4508\n",
      "Epoch: 668 Cost: 25.4504\n",
      "Epoch: 669 Cost: 25.4499\n",
      "Epoch: 670 Cost: 25.4494\n",
      "Epoch: 671 Cost: 25.449\n",
      "Epoch: 672 Cost: 25.4485\n",
      "Epoch: 673 Cost: 25.4481\n",
      "Epoch: 674 Cost: 25.4476\n",
      "Epoch: 675 Cost: 25.4472\n",
      "Epoch: 676 Cost: 25.4467\n",
      "Epoch: 677 Cost: 25.4463\n",
      "Epoch: 678 Cost: 25.4459\n",
      "Epoch: 679 Cost: 25.4454\n",
      "Epoch: 680 Cost: 25.445\n",
      "Epoch: 681 Cost: 25.4445\n",
      "Epoch: 682 Cost: 25.4441\n",
      "Epoch: 683 Cost: 25.4437\n",
      "Epoch: 684 Cost: 25.4433\n",
      "Epoch: 685 Cost: 25.4428\n",
      "Epoch: 686 Cost: 25.4424\n",
      "Epoch: 687 Cost: 25.442\n",
      "Epoch: 688 Cost: 25.4416\n",
      "Epoch: 689 Cost: 25.4411\n",
      "Epoch: 690 Cost: 25.4407\n",
      "Epoch: 691 Cost: 25.4403\n",
      "Epoch: 692 Cost: 25.4399\n",
      "Epoch: 693 Cost: 25.4395\n",
      "Epoch: 694 Cost: 25.4391\n",
      "Epoch: 695 Cost: 25.4387\n",
      "Epoch: 696 Cost: 25.4383\n",
      "Epoch: 697 Cost: 25.4379\n",
      "Epoch: 698 Cost: 25.4375\n",
      "Epoch: 699 Cost: 25.4371\n",
      "Epoch: 700 Cost: 25.4367\n",
      "Epoch: 701 Cost: 25.4363\n",
      "Epoch: 702 Cost: 25.4359\n",
      "Epoch: 703 Cost: 25.4355\n",
      "Epoch: 704 Cost: 25.4351\n",
      "Epoch: 705 Cost: 25.4347\n",
      "Epoch: 706 Cost: 25.4343\n",
      "Epoch: 707 Cost: 25.434\n",
      "Epoch: 708 Cost: 25.4336\n",
      "Epoch: 709 Cost: 25.4332\n",
      "Epoch: 710 Cost: 25.4328\n",
      "Epoch: 711 Cost: 25.4324\n",
      "Epoch: 712 Cost: 25.4321\n",
      "Epoch: 713 Cost: 25.4317\n",
      "Epoch: 714 Cost: 25.4313\n",
      "Epoch: 715 Cost: 25.431\n",
      "Epoch: 716 Cost: 25.4306\n",
      "Epoch: 717 Cost: 25.4302\n",
      "Epoch: 718 Cost: 25.4299\n",
      "Epoch: 719 Cost: 25.4295\n",
      "Epoch: 720 Cost: 25.4291\n",
      "Epoch: 721 Cost: 25.4288\n",
      "Epoch: 722 Cost: 25.4284\n",
      "Epoch: 723 Cost: 25.4281\n",
      "Epoch: 724 Cost: 25.4277\n",
      "Epoch: 725 Cost: 25.4274\n",
      "Epoch: 726 Cost: 25.427\n",
      "Epoch: 727 Cost: 25.4267\n",
      "Epoch: 728 Cost: 25.4263\n",
      "Epoch: 729 Cost: 25.426\n",
      "Epoch: 730 Cost: 25.4256\n",
      "Epoch: 731 Cost: 25.4253\n",
      "Epoch: 732 Cost: 25.425\n",
      "Epoch: 733 Cost: 25.4246\n",
      "Epoch: 734 Cost: 25.4243\n",
      "Epoch: 735 Cost: 25.4239\n",
      "Epoch: 736 Cost: 25.4236\n",
      "Epoch: 737 Cost: 25.4233\n",
      "Epoch: 738 Cost: 25.423\n",
      "Epoch: 739 Cost: 25.4226\n",
      "Epoch: 740 Cost: 25.4223\n",
      "Epoch: 741 Cost: 25.422\n",
      "Epoch: 742 Cost: 25.4216\n",
      "Epoch: 743 Cost: 25.4213\n",
      "Epoch: 744 Cost: 25.421\n",
      "Epoch: 745 Cost: 25.4207\n",
      "Epoch: 746 Cost: 25.4204\n",
      "Epoch: 747 Cost: 25.42\n",
      "Epoch: 748 Cost: 25.4197\n",
      "Epoch: 749 Cost: 25.4194\n",
      "Epoch: 750 Cost: 25.4191\n",
      "Epoch: 751 Cost: 25.4188\n",
      "Epoch: 752 Cost: 25.4185\n",
      "Epoch: 753 Cost: 25.4182\n",
      "Epoch: 754 Cost: 25.4179\n",
      "Epoch: 755 Cost: 25.4176\n",
      "Epoch: 756 Cost: 25.4172\n",
      "Epoch: 757 Cost: 25.417\n",
      "Epoch: 758 Cost: 25.4167\n",
      "Epoch: 759 Cost: 25.4163\n",
      "Epoch: 760 Cost: 25.416\n",
      "Epoch: 761 Cost: 25.4158\n",
      "Epoch: 762 Cost: 25.4155\n",
      "Epoch: 763 Cost: 25.4152\n",
      "Epoch: 764 Cost: 25.4149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 765 Cost: 25.4146\n",
      "Epoch: 766 Cost: 25.4143\n",
      "Epoch: 767 Cost: 25.414\n",
      "Epoch: 768 Cost: 25.4137\n",
      "Epoch: 769 Cost: 25.4134\n",
      "Epoch: 770 Cost: 25.4131\n",
      "Epoch: 771 Cost: 25.4128\n",
      "Epoch: 772 Cost: 25.4126\n",
      "Epoch: 773 Cost: 25.4123\n",
      "Epoch: 774 Cost: 25.412\n",
      "Epoch: 775 Cost: 25.4117\n",
      "Epoch: 776 Cost: 25.4114\n",
      "Epoch: 777 Cost: 25.4112\n",
      "Epoch: 778 Cost: 25.4109\n",
      "Epoch: 779 Cost: 25.4106\n",
      "Epoch: 780 Cost: 25.4103\n",
      "Epoch: 781 Cost: 25.4101\n",
      "Epoch: 782 Cost: 25.4098\n",
      "Epoch: 783 Cost: 25.4095\n",
      "Epoch: 784 Cost: 25.4093\n",
      "Epoch: 785 Cost: 25.409\n",
      "Epoch: 786 Cost: 25.4087\n",
      "Epoch: 787 Cost: 25.4085\n",
      "Epoch: 788 Cost: 25.4082\n",
      "Epoch: 789 Cost: 25.4079\n",
      "Epoch: 790 Cost: 25.4077\n",
      "Epoch: 791 Cost: 25.4074\n",
      "Epoch: 792 Cost: 25.4072\n",
      "Epoch: 793 Cost: 25.4069\n",
      "Epoch: 794 Cost: 25.4066\n",
      "Epoch: 795 Cost: 25.4064\n",
      "Epoch: 796 Cost: 25.4061\n",
      "Epoch: 797 Cost: 25.4059\n",
      "Epoch: 798 Cost: 25.4056\n",
      "Epoch: 799 Cost: 25.4054\n",
      "Epoch: 800 Cost: 25.4051\n",
      "Epoch: 801 Cost: 25.4049\n",
      "Epoch: 802 Cost: 25.4046\n",
      "Epoch: 803 Cost: 25.4044\n",
      "Epoch: 804 Cost: 25.4041\n",
      "Epoch: 805 Cost: 25.4039\n",
      "Epoch: 806 Cost: 25.4036\n",
      "Epoch: 807 Cost: 25.4034\n",
      "Epoch: 808 Cost: 25.4032\n",
      "Epoch: 809 Cost: 25.4029\n",
      "Epoch: 810 Cost: 25.4027\n",
      "Epoch: 811 Cost: 25.4025\n",
      "Epoch: 812 Cost: 25.4022\n",
      "Epoch: 813 Cost: 25.402\n",
      "Epoch: 814 Cost: 25.4017\n",
      "Epoch: 815 Cost: 25.4015\n",
      "Epoch: 816 Cost: 25.4013\n",
      "Epoch: 817 Cost: 25.401\n",
      "Epoch: 818 Cost: 25.4008\n",
      "Epoch: 819 Cost: 25.4006\n",
      "Epoch: 820 Cost: 25.4004\n",
      "Epoch: 821 Cost: 25.4001\n",
      "Epoch: 822 Cost: 25.3999\n",
      "Epoch: 823 Cost: 25.3997\n",
      "Epoch: 824 Cost: 25.3994\n",
      "Epoch: 825 Cost: 25.3992\n",
      "Epoch: 826 Cost: 25.399\n",
      "Epoch: 827 Cost: 25.3988\n",
      "Epoch: 828 Cost: 25.3985\n",
      "Epoch: 829 Cost: 25.3983\n",
      "Epoch: 830 Cost: 25.3981\n",
      "Epoch: 831 Cost: 25.3979\n",
      "Epoch: 832 Cost: 25.3977\n",
      "Epoch: 833 Cost: 25.3974\n",
      "Epoch: 834 Cost: 25.3972\n",
      "Epoch: 835 Cost: 25.397\n",
      "Epoch: 836 Cost: 25.3968\n",
      "Epoch: 837 Cost: 25.3966\n",
      "Epoch: 838 Cost: 25.3964\n",
      "Epoch: 839 Cost: 25.3962\n",
      "Epoch: 840 Cost: 25.396\n",
      "Epoch: 841 Cost: 25.3957\n",
      "Epoch: 842 Cost: 25.3955\n",
      "Epoch: 843 Cost: 25.3953\n",
      "Epoch: 844 Cost: 25.3951\n",
      "Epoch: 845 Cost: 25.3949\n",
      "Epoch: 846 Cost: 25.3947\n",
      "Epoch: 847 Cost: 25.3945\n",
      "Epoch: 848 Cost: 25.3943\n",
      "Epoch: 849 Cost: 25.3941\n",
      "Epoch: 850 Cost: 25.3939\n",
      "Epoch: 851 Cost: 25.3937\n",
      "Epoch: 852 Cost: 25.3935\n",
      "Epoch: 853 Cost: 25.3933\n",
      "Epoch: 854 Cost: 25.3931\n",
      "Epoch: 855 Cost: 25.3929\n",
      "Epoch: 856 Cost: 25.3927\n",
      "Epoch: 857 Cost: 25.3925\n",
      "Epoch: 858 Cost: 25.3923\n",
      "Epoch: 859 Cost: 25.3921\n",
      "Epoch: 860 Cost: 25.3919\n",
      "Epoch: 861 Cost: 25.3917\n",
      "Epoch: 862 Cost: 25.3915\n",
      "Epoch: 863 Cost: 25.3913\n",
      "Epoch: 864 Cost: 25.3911\n",
      "Epoch: 865 Cost: 25.391\n",
      "Epoch: 866 Cost: 25.3908\n",
      "Epoch: 867 Cost: 25.3906\n",
      "Epoch: 868 Cost: 25.3904\n",
      "Epoch: 869 Cost: 25.3902\n",
      "Epoch: 870 Cost: 25.39\n",
      "Epoch: 871 Cost: 25.3898\n",
      "Epoch: 872 Cost: 25.3896\n",
      "Epoch: 873 Cost: 25.3895\n",
      "Epoch: 874 Cost: 25.3893\n",
      "Epoch: 875 Cost: 25.3891\n",
      "Epoch: 876 Cost: 25.3889\n",
      "Epoch: 877 Cost: 25.3887\n",
      "Epoch: 878 Cost: 25.3885\n",
      "Epoch: 879 Cost: 25.3884\n",
      "Epoch: 880 Cost: 25.3882\n",
      "Epoch: 881 Cost: 25.388\n",
      "Epoch: 882 Cost: 25.3878\n",
      "Epoch: 883 Cost: 25.3877\n",
      "Epoch: 884 Cost: 25.3875\n",
      "Epoch: 885 Cost: 25.3873\n",
      "Epoch: 886 Cost: 25.3871\n",
      "Epoch: 887 Cost: 25.387\n",
      "Epoch: 888 Cost: 25.3868\n",
      "Epoch: 889 Cost: 25.3866\n",
      "Epoch: 890 Cost: 25.3864\n",
      "Epoch: 891 Cost: 25.3863\n",
      "Epoch: 892 Cost: 25.3861\n",
      "Epoch: 893 Cost: 25.3859\n",
      "Epoch: 894 Cost: 25.3857\n",
      "Epoch: 895 Cost: 25.3856\n",
      "Epoch: 896 Cost: 25.3854\n",
      "Epoch: 897 Cost: 25.3852\n",
      "Epoch: 898 Cost: 25.3851\n",
      "Epoch: 899 Cost: 25.3849\n",
      "Epoch: 900 Cost: 25.3848\n",
      "Epoch: 901 Cost: 25.3846\n",
      "Epoch: 902 Cost: 25.3844\n",
      "Epoch: 903 Cost: 25.3843\n",
      "Epoch: 904 Cost: 25.3841\n",
      "Epoch: 905 Cost: 25.3839\n",
      "Epoch: 906 Cost: 25.3838\n",
      "Epoch: 907 Cost: 25.3836\n",
      "Epoch: 908 Cost: 25.3834\n",
      "Epoch: 909 Cost: 25.3833\n",
      "Epoch: 910 Cost: 25.3831\n",
      "Epoch: 911 Cost: 25.383\n",
      "Epoch: 912 Cost: 25.3828\n",
      "Epoch: 913 Cost: 25.3827\n",
      "Epoch: 914 Cost: 25.3825\n",
      "Epoch: 915 Cost: 25.3823\n",
      "Epoch: 916 Cost: 25.3822\n",
      "Epoch: 917 Cost: 25.382\n",
      "Epoch: 918 Cost: 25.3819\n",
      "Epoch: 919 Cost: 25.3817\n",
      "Epoch: 920 Cost: 25.3816\n",
      "Epoch: 921 Cost: 25.3814\n",
      "Epoch: 922 Cost: 25.3813\n",
      "Epoch: 923 Cost: 25.3811\n",
      "Epoch: 924 Cost: 25.381\n",
      "Epoch: 925 Cost: 25.3808\n",
      "Epoch: 926 Cost: 25.3807\n",
      "Epoch: 927 Cost: 25.3805\n",
      "Epoch: 928 Cost: 25.3804\n",
      "Epoch: 929 Cost: 25.3802\n",
      "Epoch: 930 Cost: 25.3801\n",
      "Epoch: 931 Cost: 25.3799\n",
      "Epoch: 932 Cost: 25.3798\n",
      "Epoch: 933 Cost: 25.3796\n",
      "Epoch: 934 Cost: 25.3795\n",
      "Epoch: 935 Cost: 25.3793\n",
      "Epoch: 936 Cost: 25.3792\n",
      "Epoch: 937 Cost: 25.3791\n",
      "Epoch: 938 Cost: 25.3789\n",
      "Epoch: 939 Cost: 25.3788\n",
      "Epoch: 940 Cost: 25.3786\n",
      "Epoch: 941 Cost: 25.3785\n",
      "Epoch: 942 Cost: 25.3783\n",
      "Epoch: 943 Cost: 25.3782\n",
      "Epoch: 944 Cost: 25.3781\n",
      "Epoch: 945 Cost: 25.3779\n",
      "Epoch: 946 Cost: 25.3778\n",
      "Epoch: 947 Cost: 25.3776\n",
      "Epoch: 948 Cost: 25.3775\n",
      "Epoch: 949 Cost: 25.3774\n",
      "Epoch: 950 Cost: 25.3772\n",
      "Epoch: 951 Cost: 25.3771\n",
      "Epoch: 952 Cost: 25.377\n",
      "Epoch: 953 Cost: 25.3768\n",
      "Epoch: 954 Cost: 25.3767\n",
      "Epoch: 955 Cost: 25.3766\n",
      "Epoch: 956 Cost: 25.3764\n",
      "Epoch: 957 Cost: 25.3763\n",
      "Epoch: 958 Cost: 25.3762\n",
      "Epoch: 959 Cost: 25.376\n",
      "Epoch: 960 Cost: 25.3759\n",
      "Epoch: 961 Cost: 25.3758\n",
      "Epoch: 962 Cost: 25.3756\n",
      "Epoch: 963 Cost: 25.3755\n",
      "Epoch: 964 Cost: 25.3754\n",
      "Epoch: 965 Cost: 25.3752\n",
      "Epoch: 966 Cost: 25.3751\n",
      "Epoch: 967 Cost: 25.375\n",
      "Epoch: 968 Cost: 25.3749\n",
      "Epoch: 969 Cost: 25.3747\n",
      "Epoch: 970 Cost: 25.3746\n",
      "Epoch: 971 Cost: 25.3745\n",
      "Epoch: 972 Cost: 25.3743\n",
      "Epoch: 973 Cost: 25.3742\n",
      "Epoch: 974 Cost: 25.3741\n",
      "Epoch: 975 Cost: 25.374\n",
      "Epoch: 976 Cost: 25.3738\n",
      "Epoch: 977 Cost: 25.3737\n",
      "Epoch: 978 Cost: 25.3736\n",
      "Epoch: 979 Cost: 25.3735\n",
      "Epoch: 980 Cost: 25.3734\n",
      "Epoch: 981 Cost: 25.3732\n",
      "Epoch: 982 Cost: 25.3731\n",
      "Epoch: 983 Cost: 25.373\n",
      "Epoch: 984 Cost: 25.3729\n",
      "Epoch: 985 Cost: 25.3727\n",
      "Epoch: 986 Cost: 25.3726\n",
      "Epoch: 987 Cost: 25.3725\n",
      "Epoch: 988 Cost: 25.3724\n",
      "Epoch: 989 Cost: 25.3723\n",
      "Epoch: 990 Cost: 25.3722\n",
      "Epoch: 991 Cost: 25.372\n",
      "Epoch: 992 Cost: 25.3719\n",
      "Epoch: 993 Cost: 25.3718\n",
      "Epoch: 994 Cost: 25.3717\n",
      "Epoch: 995 Cost: 25.3716\n",
      "Epoch: 996 Cost: 25.3714\n",
      "Epoch: 997 Cost: 25.3713\n",
      "Epoch: 998 Cost: 25.3712\n",
      "Epoch: 999 Cost: 25.3711\n",
      "Test Loss: 30.3771\n"
     ]
    }
   ],
   "source": [
    "num_epochs  = 1000\n",
    "losses = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs):\n",
    "        _, c = sess.run([update, loss], feed_dict={X: X_train, \n",
    "                                                      Y: y_train})\n",
    "        losses.append(c)\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",c)\n",
    "    test_loss = sess.run(loss, feed_dict={X: X_test, \n",
    "                                          Y: y_test})\n",
    "    print(\"Test Loss:\",test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x181cecb048>]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181c8ff748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(test_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
