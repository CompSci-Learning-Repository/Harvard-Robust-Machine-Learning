{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Classification, Regression\n",
    "This section will cover logistic regression (which is a classification model, not regression) and linear regression. Once you can train both of these models, you know how to do both classification and regression in their simplest way possible. I first demonstrate how to train a logistic regression using the Iris dataset. As an exercise, you will train a linear regression on the Boston housing dataset.\n",
    "\n",
    "This section will put together what we've learned so far, e.g. `session`, `placeholder`, `variable`, etc. This section might seem hard at first, but the only new concept that will be introduced here is the `GradientDescentOptimizer`. As long as you understand the code step by step, it should be ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "This example is partially adopted from [Deep MNIST for Experts](https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/) tutorial from TensorFlow official website.\n",
    "\n",
    "### Task\n",
    "\n",
    "Classify a dataset of hand-written digits (0 through 9).\n",
    "\n",
    "### Data\n",
    "\n",
    "Let's load the data first. Thankfully, TensorFlow provided us a nice API. The data is also already split into train, validation and test set. In general, the data is much less clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first image to get a sense of what is inside. Note that `next_batch` is an API provided by TensorFlow to get the next `n` images from the dataset. You generally have to code this up yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.01568628,  0.        ,  0.        ,\n",
       "         0.03137255,  0.03137255,  0.11764707,  0.42745101,  0.04313726,\n",
       "         0.04313726,  0.04313726,  0.03921569,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.30980393,\n",
       "         0.7843138 ,  0.64705884,  0.64705884,  0.92549026,  0.91764712,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.96470594,  0.64705884,  0.15294118,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.08235294,  0.47450984,  0.87450987,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.92549026,  0.91764712,  0.68627453,  0.53333336,\n",
       "         0.82745105,  0.54901963,  0.94117653,  0.99607849,  0.99607849,\n",
       "         0.92549026,  0.40784317,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.15294118,  0.9450981 ,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.52941179,  0.3137255 ,  0.01960784,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.08627451,  0.4666667 ,  0.58823532,  0.8588236 ,  0.88627458,\n",
       "         0.14117648,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.77254909,  0.72941178,  0.81960791,  0.99607849,  0.72941178,\n",
       "         0.01176471,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.3137255 ,  0.96862751,  0.7843138 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.55686277,  0.04705883,\n",
       "         0.36470589,  0.99607849,  0.91764712,  0.07058824,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.46274513,  0.94901967,  0.44313729,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.02352941,  0.00784314,  0.68235296,  0.99607849,\n",
       "         0.65882355,  0.00784314,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.01960784,  0.96470594,\n",
       "         0.97647065,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.04313726,  0.99607849,  0.99607849,  0.3137255 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.96078438,  0.99607849,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.42745101,  0.99607849,\n",
       "         0.95294124,  0.12941177,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.07058824,\n",
       "         0.96862751,  0.76470596,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.43529415,  0.99607849,  0.91764712,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.36470589,  0.99607849,  0.44705886,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.43529415,\n",
       "         0.99607849,  0.91764712,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.03529412,\n",
       "         0.80392164,  0.99607849,  0.21568629,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.43529415,  0.99607849,  0.91764712,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.16470589,  0.99607849,  0.91764712,\n",
       "         0.11764707,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.17254902,  0.99607849,  0.97254908,  0.31764707,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.09019608,\n",
       "         0.81176478,  0.99607849,  0.45098042,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00784314,  0.64705884,\n",
       "         0.99607849,  0.84705889,  0.04705883,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.28235295,  0.80392164,  0.98431379,  0.76078439,\n",
       "         0.01960784,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.25882354,  0.99607849,  0.99607849,\n",
       "         0.53333336,  0.08627451,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.14509805,  0.57647061,  0.96470594,\n",
       "         0.89803928,  0.43921572,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.01960784,  0.61176473,  0.99607849,  0.99607849,  0.9450981 ,\n",
       "         0.70588237,  0.52941179,  0.52941179,  0.69411767,  0.92156869,\n",
       "         0.95686281,  0.99607849,  0.98431379,  0.43921572,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.02352941,\n",
       "         0.36862746,  0.82352948,  0.99607849,  0.99607849,  0.99607849,\n",
       "         0.99607849,  0.99607849,  0.99607849,  0.97647065,  0.64313728,\n",
       "         0.27450982,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01960784,\n",
       "         0.03921569,  0.40784317,  0.43137258,  0.43137258,  0.43137258,\n",
       "         0.33725491,  0.03529412,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = mnist.train.next_batch(1) # get one image from the data\n",
    "plotData = batch[0]\n",
    "plotData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $28\\times 28=784$ pixels. $0$ means black and $1$ means white. Let's reshape it and use `matplotlib` to make it an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1813794fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotData = plotData.reshape(28, 28) # reshape to make image a matirx\n",
    "plt.gray() # use this line if you don't want to see it in color\n",
    "plt.imshow(plotData)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a softmax regression. Softmax regression is just a multi-class generalization of the logistic regression model:\n",
    "\n",
    "$Y=softmax(WX+b)$\n",
    "where\n",
    "\n",
    "$Y: N \\times K$. Labels. The labels are one-hot encoded.\n",
    "\n",
    "$X: N \\times M$. Features.\n",
    "\n",
    "$W: M \\times N$. Weights.\n",
    "\n",
    "$b: K$. Bias\n",
    "\n",
    "Here, $N$ can either be the train, validation, test sample size or the mini-batch size, depending on which part of the process you're in. $M$ is the number of features (784 in this case) and $K$ is the number of classes (10 in this case).\n",
    "\n",
    "I really like to write down all the matrix dimensions before I start any coding. This way, I can avoid matrix dimension errors later on. This is kind of like creating an outline of your essay before you start drafting it.\n",
    "\n",
    "The loss we would like to optimize is the cross-entropy loss, averaged over the sample size:\n",
    "\n",
    "$$\\frac{-\\sum_{n=1}^N \\sum_{k=1}^K y_{n,k} log(\\hat{y}_{n,k})}{N}$$\n",
    "\n",
    "We will train this using gradient descent. \n",
    "\n",
    "Coding of this model is a six step process:\n",
    "1. Define variables and placeholders.\n",
    "2. Define the model.\n",
    "3. Define the loss function.\n",
    "4. Define the optimizer.\n",
    "5. Train the model, i.e. initialize variables and run optimizer.\n",
    "6. Evaluate the model.\n",
    "\n",
    "This is the same for any other models. Neural nets will have these steps too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y,X$ will be a placeholder, because we will feed in datasets with different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 784\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, M])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W,b$ will be variables, because we would like to mutate them every epoch via the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([M, K])) # the variables will be all zeros initially\n",
    "b = tf.Variable(tf.zeros([K]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Softmax regression model is a one liner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = tf.nn.softmax(X @ W + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "The cross-entropy loss is also a one liner! Make sure to compare it with the equation above and check that it's doing what it's supposed to. We will also define accuracy for evaluating the result on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss               = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(Yhat), axis=1))\n",
    "# choose predictions from 10 classes and compare them with the true labels\n",
    "correct_prediction = tf.equal(tf.argmax(Yhat, 1), tf.argmax(Y, 1))           \n",
    "accuracy           = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Here's somethig new to learn! `GradientDescentOptimizer` takes in the objective that we would like to minimize. When we run this optimizer in a session, it will look for all trainable variables (or the specified list of variables, if any) and update them. There are many other optimizers e.g. `AdagradOptimizer`, `AdamOptimizer`. Find your favorite!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01 # this is a hyperparameter to tune\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate) # first initialize an optimizer\n",
    "update = optimizer.minimize(loss) # then pass in the objective you would like to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables and run optimizer, evaluate the model\n",
    "The final step is to train the model, and then to evaluate it. We do these in one go. \n",
    "We have 25 epochs. Within each epoch, we go through the dataset batch by batch. In each bath, we `sess.run` two variables: `update` and `loss`. Running `update` applies backpropagation to the graph and updates the variables relevant to minimizing the objective. In this case `W` and `b` are updated. Running `loss` just extracts the objective so that we can keep track of how well our model is being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 1.18395111539\n",
      "Epoch: 1 Cost: 0.665367322835\n",
      "Epoch: 2 Cost: 0.552765985836\n",
      "Epoch: 3 Cost: 0.498695070256\n",
      "Epoch: 4 Cost: 0.465495848168\n",
      "Epoch: 5 Cost: 0.4425742106\n",
      "Epoch: 6 Cost: 0.425507432331\n",
      "Epoch: 7 Cost: 0.412195533135\n",
      "Epoch: 8 Cost: 0.401369705525\n",
      "Epoch: 9 Cost: 0.392432074791\n",
      "Epoch: 10 Cost: 0.384792247523\n",
      "Epoch: 11 Cost: 0.378197459416\n",
      "Epoch: 12 Cost: 0.372437060259\n",
      "Epoch: 13 Cost: 0.367369769676\n",
      "Epoch: 14 Cost: 0.362621878169\n",
      "Epoch: 15 Cost: 0.358624564409\n",
      "Epoch: 16 Cost: 0.35492008025\n",
      "Epoch: 17 Cost: 0.35138198858\n",
      "Epoch: 18 Cost: 0.348352879177\n",
      "Epoch: 19 Cost: 0.345445586416\n",
      "Epoch: 20 Cost: 0.342711562379\n",
      "Epoch: 21 Cost: 0.340259887657\n",
      "Epoch: 22 Cost: 0.337946226272\n",
      "Epoch: 23 Cost: 0.335711251822\n",
      "Epoch: 24 Cost: 0.33367082799\n",
      "Test Accuracy: 0.9136\n"
     ]
    }
   ],
   "source": [
    "num_epochs  = 25\n",
    "batch_size  = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) # remember, you need to initialize variables first\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    for epoch in range(num_epochs):\n",
    "        average_cost = 0 # print cost per epoch\n",
    "        for _ in range(total_batch):\n",
    "            batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([update, loss], feed_dict={X: batch_X, \n",
    "                                                          Y: batch_Y}) # evaluate `update` and `loss`\n",
    "            average_cost += c / total_batch\n",
    "        print(\"Epoch:\",epoch,\"Cost:\",average_cost)\n",
    "        \n",
    "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!\n",
    "We have 92% ish accuracy. How good is this? To cite the `TensorFlow` documentation:\n",
    "\n",
    "> Getting 92% accuracy on MNIST is bad. It's almost embarrassingly bad. In this section, we'll fix that, jumping from a very simple model to something moderately sophisticated: a small convolutional neural network. This will get us to around 99.2% accuracy -- not state of the art, but respectable.\n",
    "\n",
    "We are not going to introduce CNNs in this workshop, but you should take a look to aim for 99+% accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Train a linear regression using gradient descent (no closed form!) that predicts housing prices in Boston from other covariates provided. Follow the six steps above. Plot the loss and observe that it goes down.\n",
    "\n",
    "(Optional) Fit a LASSO. Fit a Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data)\n",
    "df.columns = boston.feature_names\n",
    "df['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_workshop]",
   "language": "python",
   "name": "conda-env-tensorflow_workshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
